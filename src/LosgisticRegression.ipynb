{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights shape: (20, 61189)\n",
      "training data shape: (12000, 61189)\n",
      "training data transpose shape: (61189, 12000)\n",
      "delta shape: (20, 12000)\n",
      "14\n",
      "14\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# the number of features\n",
    "n = 61188\n",
    "#number of classes\n",
    "K = 20\n",
    "#training instances\n",
    "M = 12000\n",
    "Normalize_X = False\n",
    "\n",
    "#this will hold the documents that get mixed togerher!\n",
    "confusion_matrix = np.zeros(shape=(21, 21), dtype=np.int16)\n",
    "\n",
    "training_labels = np.load('../res/lr_labels.npy')\n",
    "#loading the normalized training data!\n",
    "training_data_sparse = sparse.load_npz('../res/lr_training_data.npz')\n",
    "if Normalize_X:\n",
    "    training_data_sparse = training_data_sparse.multiply(training_data_sparse.sum(axis=0)).tocsr()\n",
    "XT = training_data_sparse.transpose()\n",
    "weights_matrix = np.random.rand(K, n + 1)\n",
    "# weights_sparse is the W matrix in the pdf\n",
    "weights_sparse = sparse.csr_matrix(weights_matrix, dtype=np.float)\n",
    "print('weights shape:', weights_sparse.shape)\n",
    "print('training data shape:', training_data_sparse.shape)\n",
    "print('training data transpose shape:', XT.shape)\n",
    "\n",
    "\n",
    "delta = lil_matrix(np.zeros((K, M)), dtype = np.int16)\n",
    "for i, label in enumerate(training_labels):\n",
    "    delta[label - 1, i] = 1\n",
    "delta = delta.tocsr()\n",
    "print('delta shape:', delta.shape)\n",
    "# you see the index 1 mapping here: we need to be careful!\n",
    "#how to get the true class of a training examples\n",
    "print(delta[:,0].nonzero()[0][0] + 1)\n",
    "print(training_labels[0])\n",
    "print(sparse.issparse(delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD WEIGHT MATRIX IF YOU WANT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weight_sparse(file_name):\n",
    "    global weights_sparse\n",
    "    weights_sparse = sparse.load_npz(file_name)\n",
    "# example : \n",
    "load_weight_sparse(\"../res/weights_0.001_0.002_3000.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETHA = 0.005\n",
    "LAMBDA = 0.01\n",
    "EPOCH = 3000\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Started at:\", t0)\n",
    "for i in range(EPOCH):\n",
    "    # print(weights_sparse)    \n",
    "    pxy = weights_sparse * XT\n",
    "    #normalize before exp\n",
    "    pxy = pxy.multiply(1/pxy.sum(axis=0)).tocsr()\n",
    "    pxy = pxy.expm1().tocsr()\n",
    "    pxy = pxy.multiply(1/pxy.sum(axis=0)).tocsr()\n",
    "    diff_exp = delta - pxy\n",
    "    new_w = weights_sparse + ETHA * (diff_exp * training_data_sparse - weights_sparse.multiply(LAMBDA))\n",
    "    weights_sparse = new_w\n",
    "    if i % 100 == 0:\n",
    "        print(f\"epoch {i}\")\n",
    "    i += 1\n",
    "    # np.nan_to_num(weights_sparse, copy=False)\n",
    "\n",
    "t1 = time.time()\n",
    "print('Finished at:', t1, \"\\nTotal runtime:\", t1 - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Weights if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE WEIGHTS\n",
    "def save_weight(name=\"\"):\n",
    "    if not name:\n",
    "        import datetime\n",
    "        name = \"../res/weight_matrix\" + str(datetime.datatime.now())\n",
    "    else:\n",
    "        name = \"../res/\" + name\n",
    "    sparse.save_npz(name, weights_sparse)\n",
    "save_weight(\"weights_1000_runs_acc_85\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights_sparse.shape)                if actual < predicted:\n",
    "                    min_idx = \n",
    "print(weights_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_weights(predictions, is_training=False):\n",
    "    correct = 0\n",
    "    predictions = predictions.multiply(1/predictions.sum(axis=0))\n",
    "    predictions = predictions.expm1()\n",
    "    predictions = predictions.multiply(1/predictions.sum(axis=0))\n",
    "    predictions = predictions.todense()\n",
    "    predictions_output = np.zeros(shape=predictions.shape[1], dtype=np.int8)\n",
    "    truth_output = np.zeros(shape=predictions.shape[1], dtype=np.int8)\n",
    "    if is_training:\n",
    "        for i in range(predictions.shape[1]):\n",
    "            predicted = np.argmax(predictions[:,i]) + 1\n",
    "            actual = training_labels[i]\n",
    "            truth_output[i] = actual\n",
    "            predictions_output[i] = predicted \n",
    "            if predicted == actual:\n",
    "                correct += 1\n",
    "            else:\n",
    "                #doing the confusion matrix\n",
    "                min_idx = predicted\n",
    "                #the row says the real class the column says what class it was confused wth\n",
    "                #when reading the table, start index from 1 in your head!\n",
    "                confusion_matrix[actual, predicted] += 1\n",
    "        print(f\"Correct is {correct} and accuracy is {correct * 100/predictions.shape[1]}%\")\n",
    "        print(\"confusion matrix:\")\n",
    "        for i in range(1, confusion_matrix.shape[1]):\n",
    "            print(i, end=\"->\")\n",
    "            print(confusion_matrix[i,1:])\n",
    "    else:\n",
    "        print('writing to file...')\n",
    "        with open('../results/lr_out.csv', 'w') as out_stream:\n",
    "            out_stream.write(\"id,class\\n\")\n",
    "            ids = range(12001, 18775)\n",
    "            for i in range(predictions.shape[1]):\n",
    "                predicted_label = np.argmax(predictions[:,i]) + 1\n",
    "                doc_id = ids[i]\n",
    "                out_stream.write(f\"{doc_id},{predicted_label}\\n\")\n",
    "    return predictions_output, truth_output\n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct is 11036 and accuracy is 91.96666666666667%\n",
      "confusion matrix:\n",
      "1->[  0   0   0   0   0   0   0  20   0   0   0   0   0  10  20 140  10  60\n",
      "  60 100]\n",
      "2->[ 30   0 230  80  40 110  40   0   0  10  20  10  40  20  40  30   0   0\n",
      "  10  30]\n",
      "3->[ 20 140   0 200  70 130  20   0   0  10   0  10  10  10  20  10  10   0\n",
      "  40  20]\n",
      "4->[ 20 130 250   0  60  40  90  30   0  10  10  40  80  10   0  10  20   0\n",
      "   0   0]\n",
      "5->[ 10  40  70 240   0  20  60   0  10   0   0  10 100  20  60  10   0   0\n",
      "  20   0]\n",
      "6->[ 20 180 250  40  20   0  20   0   0  10   0  10  30  40  50   0   0  10\n",
      "   0   0]\n",
      "7->[  0  10  30 190 130  40   0 130  20  10  30  30  80   0  30  20  30   0\n",
      "  30  20]\n",
      "8->[20  0 20 20 20 20 80  0 90 10  0  0 60  0 20 10 10 30 20 20]\n",
      "9->[  0   0   0   0  10   0  40 140   0  10   0   0  20  10   0  10  10  20\n",
      "   0   0]\n",
      "10->[10  0  0  0  0 10 10 10 10  0 80 10 20  0  0 10 20  0 10  0]\n",
      "11->[ 0 10  0  0  0  0 10  0  0 30  0 10 10 10 20  0  0  0  0  0]\n",
      "12->[ 0 20  0  0 10  0  0  0 10  0  0  0  0 10  0 20 50 10 10  0]\n",
      "13->[  0  60  40  80 120  20  80  90  10  20  20  60   0  30  40  20   0   0\n",
      "   0   0]\n",
      "14->[30 20  0  0  0  0 20 10 10  0  0  0 40  0 10 50 40  0 10 20]\n",
      "15->[20 30  0  0 20  0  0  0  0 20  0 10 20 40  0 10 20  0 10  0]\n",
      "16->[100  10  10   0   0  10  10   0   0   0   0  10  30  20  10   0  30  10\n",
      "   0  80]\n",
      "17->[ 30   0   0   0   0  10  20  20   0  10  20  30   0  10  30   0   0  20\n",
      " 100  20]\n",
      "18->[20 10  0  0  0  0 10 10 20  0  0 20  0  0 20 50 20  0 50 30]\n",
      "19->[ 10   0   0  10   0   0  30  10   0  10  10  40   0  40   0  40 210  70\n",
      "   0  20]\n",
      "20->[250  10   0   0   0   0  10   0   0   0   0   0   0   0  40 410 190  80\n",
      "  70   0]\n",
      "\n",
      "[ 441   0   0   0   0   0   0   2   0   0   0   0   0   1   2  14   1   6   6  10 ]\n",
      "[   3 550  23   8   4  11   4   0   0   1   2   1   4   2   4   3   0   0   1   3 ]\n",
      "[   2  14 550  20   7  13   2   0   0   1   0   1   1   1   2   1   1   0   4   2 ]\n",
      "[   2  13  25 563   6   4   9   3   0   1   1   4   8   1   0   1   2   0   0   0 ]\n",
      "[   1   4   7  24 535   2   6   0   1   0   0   1  10   2   6   1   0   0   2   0 ]\n",
      "[   2  18  25   4   2 562   2   0   0   1   0   1   3   4   5   0   0   1   0   0 ]\n",
      "[   0   1   3  19  13   4 535  13   2   1   3   3   8   0   3   2   3   0   3   2 ]\n",
      "[   2   0   2   2   2   2   8 569   9   1   0   0   6   0   2   1   1   3   2   2 ]\n",
      "[   0   0   0   0   1   0   4  14 622   1   0   0   2   1   0   1   1   2   0   0 ]\n",
      "[   1   0   0   0   0   1   1   1   1 608   8   1   2   0   0   1   2   0   1   0 ]\n",
      "[   0   1   0   0   0   0   1   0   0   3 636   1   1   1   2   0   0   0   0   0 ]\n",
      "[   0   2   0   0   1   0   0   0   1   0   0 625   0   1   0   2   5   1   1   0 ]\n",
      "[   0   6   4   8  12   2   8   9   1   2   2   6 557   3   4   2   0   0   0   0 ]\n",
      "[   3   2   0   0   0   0   2   1   1   0   0   0   4 595   1   5   4   0   1   2 ]\n",
      "[   2   3   0   0   2   0   0   0   0   2   0   1   2   4 617   1   2   0   1   0 ]\n",
      "[  10   1   1   0   0   1   1   0   0   0   0   1   3   2   1 618   3   1   0   8 ]\n",
      "[   3   0   0   0   0   1   2   2   0   1   2   3   0   1   3   0 548   2  10   2 ]\n",
      "[   2   1   0   0   0   0   1   1   2   0   0   2   0   0   2   5   2 567   5   3 ]\n",
      "[   1   0   0   1   0   0   3   1   0   1   1   4   0   4   0   4  21   7 417   2 ]\n",
      "[  25   1   0   0   0   0   1   0   0   0   0   0   0   0   4  41  19   8   7 321 ]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "training_predictions = (weights_sparse * XT)\n",
    "pred_output, truth_output = test_weights(training_predictions, is_training=True)\n",
    "cm = metrics.confusion_matrix(truth_output, pred_output)\n",
    "print()\n",
    "for row in cm:\n",
    "    print(\"[ \", end=\"\")\n",
    "    for value in row:\n",
    "        print(\"%3d\" % value, end=\" \")\n",
    "    print(\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data_sparse = sparse.load_npz('../res/lr_testing_data.npz')\n",
    "test_XT = testing_data_sparse.transpose()\n",
    "test_weights(weights_sparse * test_XT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('weights shape:', weights_sparse.shape)\n",
    "print('testing data shape:', testing_data_sparse.shape)\n",
    "print('testing data first column:', testing_data_sparse[:,0])\n",
    "print('training data first column:', training_data_sparse[:,0])\n",
    "# print('testing data transpose shape:', test_XT.shape)\n",
    "# testing_predictions = (weights_sparse * test_XT).todense()\n",
    "# test_weights(testing_predictions, is_training=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
