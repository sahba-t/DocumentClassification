{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load csv into dense matrix, then converts it to sparse matrix\n",
    "If \"res/sparse_training.data\" exists, **DO NOT** run cell as it will take awhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dense_matrix = np.zeros(shape=(21, 61188), dtype=np.int16)\n",
    "with open('../res/training.csv', 'r') as train_stream:\n",
    "    for i, line in enumerate(train_stream):\n",
    "        line_int = np.array(list(map(int, line.split(','))), dtype=np.int16)\n",
    "        doc_label = line_int[-1]\n",
    "        dense_matrix[doc_label] += line_int[1:-1]\n",
    "        print(i)\n",
    "    sparse_training = sparse.csr_matrix(dense_matrix)\n",
    "    sparse.save_npz('../res/sparse_training.data',sparse_training)\n",
    "    print(sparse_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "  (1, 0)\t9\n",
      "  (1, 1)\t53\n",
      "  (1, 2)\t237\n",
      "  (1, 3)\t11\n",
      "  (1, 4)\t48\n",
      "  (1, 5)\t36\n",
      "  (1, 6)\t7\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t31\n",
      "  (1, 9)\t127\n",
      "  (1, 10)\t5\n",
      "  (1, 11)\t4336\n",
      "  (1, 12)\t19\n",
      "  (1, 13)\t24\n",
      "  (1, 14)\t46\n",
      "  (1, 15)\t597\n",
      "  (1, 16)\t267\n",
      "  (1, 17)\t12\n",
      "  (1, 18)\t9\n",
      "  (1, 19)\t15\n",
      "  (1, 20)\t2\n",
      "  (1, 21)\t3\n",
      "  (1, 22)\t2985\n",
      "  (1, 23)\t4\n",
      "  (1, 24)\t325\n",
      "  :\t:\n",
      "  (20, 61146)\t1\n",
      "  (20, 61147)\t1\n",
      "  (20, 61148)\t1\n",
      "  (20, 61149)\t2\n",
      "  (20, 61150)\t1\n",
      "  (20, 61151)\t1\n",
      "  (20, 61152)\t1\n",
      "  (20, 61153)\t1\n",
      "  (20, 61154)\t1\n",
      "  (20, 61169)\t2\n",
      "  (20, 61170)\t2\n",
      "  (20, 61171)\t2\n",
      "  (20, 61172)\t3\n",
      "  (20, 61173)\t4\n",
      "  (20, 61174)\t2\n",
      "  (20, 61175)\t3\n",
      "  (20, 61176)\t6\n",
      "  (20, 61177)\t1\n",
      "  (20, 61178)\t2\n",
      "  (20, 61179)\t2\n",
      "  (20, 61183)\t2\n",
      "  (20, 61184)\t2\n",
      "  (20, 61185)\t2\n",
      "  (20, 61186)\t2\n",
      "  (20, 61187)\t2\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "sparse_training_data = sparse.load_npz('../res/sparse_training.data.npz')\n",
    "# print(sparse_training_data[21, 3])  \n",
    "print(sparse_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating global vars and consts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "UNIQUE_VOCAB = 61188\n",
    "TOTAL_VOCAB = sparse_training_data.sum()\n",
    "BETA = 1/TOTAL_VOCAB\n",
    "ALPHA = 1 + BETA\n",
    "\n",
    "set_list = [set() for x in range(0, 20)]\n",
    "class_row_dict = dict(zip(list(range(1, 21)), set_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Counting Priors and words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "word_id_ranges = list(range(1, 61189))\n",
    "column_names =  ['doc_id'] + word_id_ranges + ['label']\n",
    "prior_counts = np.zeros(21, dtype=np.int16)\n",
    "i = 0\n",
    "for data_chunk in pd.read_csv('../res/training.csv', header=None, chunksize=200, names=column_names, usecols=['label']): \n",
    "    for _, row in data_chunk.iterrows():\n",
    "        current_label = row['label']\n",
    "        class_row_dict[current_label].add(i)\n",
    "        i += 1\n",
    "\n",
    "for j in range(1, 21):\n",
    "    prior_counts[j] = len(class_row_dict[j])\n",
    "\n",
    "prior_counts = prior_counts / prior_counts.sum()\n",
    "print(prior_counts) "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes\n",
    "formula from the proj2 PDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "map_denom = np.zeros(21, dtype = np.float64)\n",
    "for i in range(1, 21):\n",
    "    map_denom[i] = sparse_training_data[i].sum() + ((ALPHA - 1) * TOTAL_VOCAB)\n",
    "\n",
    "do_naive_debug = False \n",
    "def multinomial_naive_bayes(row):\n",
    "    max_prob = -math.inf\n",
    "    max_doc_class = -1\n",
    "    non_zero_indices = row.nonzero()[0]\n",
    "    for doc_label in range(1, 21):\n",
    "        running_sum = 0\n",
    "        for word in non_zero_indices:\n",
    "            running_sum += math.log2((sparse_training_data[doc_label, word] + ALPHA - 1)/map_denom[doc_label])\n",
    "        new_prob = running_sum + math.log2(prior_counts[doc_label])\n",
    "        if new_prob > max_prob:\n",
    "            max_prob = new_prob\n",
    "            max_doc_class = doc_label\n",
    "    assert(max_doc_class != -1)\n",
    "    if do_naive_debug:\n",
    "        print(f\"{max_doc_class}: {max_prob}\")\n",
    "    return max_doc_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def bernoulli_naive_bayes(row, row_total_words)->int:\n",
    "    print(row_total_words, row)\n",
    "    # num_zeros = UNIQUE_VOCAB - len(row)\n",
    "    # k = np.zeros(21, dtype=np.int16) # priors\n",
    "    # for j in range(1, 21):\n",
    "    #     k[j] = len(class_row_dict[j])\n",
    "    # \n",
    "    # denom = len(row)\n",
    "    # prob_not_appearing = (0+k) / denom\n",
    "    # # prob_of_doc = 1/20  # Not needed because all will be multiplied by it\n",
    "    # unique_words  = 0\n",
    "    # cx = 0\n",
    "    # for doc_label in range(1, 21):\n",
    "    #     if new_prob > max_prob:\n",
    "    #         max_prob = new_prob\n",
    "    #         max_doc_class = doc_label\n",
    "    return 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing\n",
    "## With training data\n",
    "### multinomial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open('../res/training.csv', 'r') as test_stream:\n",
    "    correct = 0\n",
    "\n",
    "    for line in test_stream:\n",
    "        test_array = np.array(list(map(int, line.split(','))))\n",
    "        doc_id = test_array[0]\n",
    "        doc_label = test_array[-1]\n",
    "        predicted_label = multinomial_naive_bayes(test_array[1:-1])\n",
    "        if i % 125 == 0:\n",
    "            print(f\"i={i}; pred={predicted_label}; true={doc_label}\")\n",
    "        if predicted_label == doc_label:\n",
    "            correct += 1\n",
    "    print(f\"accuracy: {(correct / 12000) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bernoulli"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "180 [(11, 5), (22, 2), (28, 13), (29, 4), (32, 10), (41, 5), (47, 2), (50, 1), (59, 4), (71, 1), (80, 4), (101, 1), (103, 5), (115, 1), (121, 1), (143, 1), (232, 1), (238, 3), (250, 3), (303, 2), (354, 1), (405, 1), (418, 1), (455, 1), (464, 1), (465, 2), (466, 2), (472, 2), (475, 1), (483, 1), (490, 1), (491, 1), (535, 1), (573, 2), (574, 1), (597, 1), (660, 1), (687, 1), (707, 1), (708, 3), (743, 1), (747, 1), (748, 3), (760, 1), (766, 1), (788, 2), (827, 1), (849, 1), (887, 1), (921, 1), (968, 1), (1002, 1), (1027, 1), (1028, 1), (1029, 1), (1035, 1), (1041, 1), (1055, 1), (1076, 1), (1088, 2), (1127, 1), (1174, 1), (1241, 1), (1434, 3), (1557, 1), (1558, 1), (1702, 1), (1820, 1), (1985, 1), (2184, 1), (2242, 1), (2578, 1), (2580, 1), (2713, 1), (3129, 1), (3148, 1), (3215, 1), (3456, 1), (4288, 1), (4312, 1), (4586, 1), (4756, 1), (4843, 1), (5372, 1), (5958, 1), (6142, 1), (6246, 1), (6480, 1), (7254, 1), (8374, 1), (9069, 1), (9514, 1), (9554, 1), (11282, 1), (11460, 1), (11903, 2), (15818, 1), (15885, 3), (16909, 1), (26375, 2), (29855, 1), (31450, 1), (32001, 1), (38596, 2), (40114, 1), (40370, 1), (41425, 1), (41668, 1), (42258, 1), (43219, 5)]\n",
      "171 [(11, 7), (26, 1), (28, 14), (29, 7), (30, 1), (32, 7), (41, 1), (47, 3), (50, 2), (51, 1), (71, 2), (72, 1), (80, 1), (98, 2), (99, 1), (101, 1), (103, 1), (118, 1), (121, 3), (142, 1), (143, 1), (182, 2), (232, 4), (238, 1), (245, 1), (250, 1), (251, 1), (294, 2), (296, 1), (300, 2), (315, 1), (319, 1), (367, 1), (421, 1), (448, 1), (470, 1), (503, 2), (654, 1), (682, 1), (687, 1), (691, 1), (721, 3), (761, 1), (774, 1), (781, 2), (784, 1), (812, 1), (824, 1), (833, 1), (849, 1), (887, 1), (921, 1), (931, 1), (950, 1), (965, 1), (992, 1), (994, 1), (1152, 1), (1180, 1), (1390, 1), (1438, 1), (1545, 1), (1648, 1), (1785, 1), (1949, 1), (2044, 1), (2416, 1), (2470, 1), (2538, 1), (2669, 1), (2684, 1), (2709, 1), (2835, 1), (3064, 1), (3198, 2), (3226, 1), (3289, 1), (3330, 1), (3679, 2), (3751, 1), (3849, 1), (4046, 1), (4384, 1), (4455, 1), (4477, 1), (4706, 1), (4940, 1), (5197, 1), (5242, 1), (5366, 1), (6419, 1), (6452, 1), (6453, 1), (7514, 1), (7580, 1), (7886, 1), (8313, 1), (8365, 1), (9871, 1), (9909, 1), (11489, 1), (12033, 1), (12677, 2), (15124, 1), (19331, 1), (22059, 4), (22111, 1), (24135, 1), (38079, 1), (43246, 1), (43248, 1), (43249, 1), (44058, 2), (46695, 1), (46905, 1), (47094, 1)]\n",
      "Finished\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "testing_data_sparse = sparse.load_npz('../res/nb_training_data.npz')\n",
    "testing_data_coo = testing_data_sparse.tocoo()\n",
    "\n",
    "row_total_words = testing_data_coo.A.sum(axis=1)\n",
    "\n",
    "correct = 0\n",
    "row = []\n",
    "for row_i, word_i, val in zip(testing_data_coo.row, testing_data_coo.col, testing_data_coo.data):\n",
    "    if row_i == 2:\n",
    "        break\n",
    "    if word_i != 61188:\n",
    "        row.append((word_i, val))\n",
    "    else:\n",
    "        classification = bernoulli_naive_bayes(row, row_total_words[row_i] - val)\n",
    "        correct += 1 if classification == val else 1\n",
    "        row.clear()\n",
    "print(\"Finished\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## With testing data\n",
    "### multinomial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "with open('../res/testing.csv', 'r') as test_stream, open('../results/out.csv', 'w') as out_stream:\n",
    "    out_stream.write(\"id,class\\n\")\n",
    "    for line in test_stream:\n",
    "        test_array = np.array(list(map(int, line.split(','))))\n",
    "        doc_id = test_array[0]\n",
    "        predicted_label = multinomial_naive_bayes(test_array[1:])    \n",
    "        out_stream.write(f\"{doc_id},{predicted_label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bernoulli"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "94 [(11, 4), (22, 5), (26, 1), (28, 13), (29, 5), (32, 3), (41, 1), (43, 1), (47, 2), (51, 1), (72, 1), (80, 2), (82, 2), (83, 2), (130, 1), (134, 1), (135, 1), (138, 2), (143, 1), (159, 2), (232, 3), (298, 1), (300, 4), (311, 1), (339, 2), (360, 1), (387, 1), (455, 1), (472, 1), (473, 3), (629, 1), (643, 8), (720, 1), (721, 1), (765, 2), (766, 1), (769, 1), (774, 2), (777, 2), (788, 1), (807, 1), (813, 1), (827, 1), (831, 1), (849, 2), (858, 1), (862, 1), (876, 1), (886, 1), (906, 1), (909, 1), (911, 1), (921, 2), (929, 2), (941, 1), (994, 3), (1032, 1), (1035, 1), (1224, 1), (1234, 1), (1409, 1), (1444, 1), (1543, 1), (1570, 2), (1640, 1), (1782, 1), (1917, 1), (1952, 1), (2122, 1), (2378, 1), (2382, 1), (2523, 1), (2538, 1), (2891, 1), (2955, 1), (3234, 1), (3244, 1), (3386, 1), (3687, 1), (4049, 2), (4091, 1), (4819, 1), (5327, 1), (5485, 1), (5534, 1), (5944, 1), (5960, 1), (6320, 1), (6478, 1), (6744, 1), (6922, 1), (6976, 2), (6997, 1), (8146, 1), (8428, 1), (8647, 1), (9030, 1), (9140, 1), (9592, 2), (9748, 1), (9809, 1), (10745, 1), (10888, 3), (13943, 1), (14804, 1), (16627, 1), (17199, 1), (19465, 1), (19589, 1), (22164, 1), (22833, 1), (23062, 1), (27107, 1), (27281, 1), (28137, 1), (28855, 1), (29024, 1), (30289, 1), (31892, 1), (32092, 1), (32664, 1), (32722, 2), (32874, 1), (57117, 1), (57181, 1)]\n",
      "56 [(22, 1), (28, 2), (32, 3), (50, 1), (51, 2), (59, 1), (71, 1), (72, 1), (79, 1), (80, 2), (121, 1), (136, 1), (191, 1), (234, 2), (250, 1), (272, 1), (296, 2), (379, 1), (392, 2), (396, 1), (468, 1), (473, 1), (554, 2), (573, 1), (625, 1), (750, 1), (782, 2), (849, 1), (968, 1), (1023, 1), (1119, 1), (1388, 2), (1602, 1), (2365, 1), (2369, 1), (2460, 1), (2656, 1), (2737, 1), (3754, 1), (3987, 1), (4257, 1), (4280, 1), (4341, 1), (5373, 1), (5534, 2), (5728, 1), (5822, 1), (6410, 1), (6524, 1), (6675, 1), (7337, 1), (7764, 1), (8106, 1), (8855, 1), (9840, 1), (10133, 1), (10471, 1), (11076, 2), (11199, 1), (12899, 1), (13333, 1), (13601, 1), (14008, 1), (14628, 1), (16421, 1), (18306, 1), (19588, 2), (21489, 1), (26970, 1), (27200, 1), (27240, 1), (28739, 1), (28757, 1), (28979, 1), (29150, 1), (30700, 1), (30811, 1), (45068, 1), (56658, 1)]\n",
      "File written\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "testing_data_sparse = sparse.load_npz('../res/nb_testing_data.npz')\n",
    "testing_data_coo = testing_data_sparse.tocoo()\n",
    "\n",
    "with open('../results/bernoulli_NB_results.csv', 'w') as out_stream:\n",
    "    out_stream.write(\"id,class\\n\")\n",
    "\n",
    "    row_total_words = testing_data_coo.A.sum(axis=1)\n",
    "    row_offset = 12001\n",
    "    current_row = 12001\n",
    "    row = []\n",
    "    for row_i, word_i, num_words_at_i in zip(testing_data_coo.row + row_offset, testing_data_coo.col, testing_data_coo.data):\n",
    "        if row_i == row_offset+3:\n",
    "            break\n",
    "        if row_i == current_row:\n",
    "            row.append((word_i, num_words_at_i))\n",
    "        else:\n",
    "            predicted_label = bernoulli_naive_bayes(row, row_total_words[row_i - row_offset])\n",
    "            out_stream.write(f'{row_i},{predicted_label}\\n')\n",
    "            row.clear()\n",
    "            current_row = row_i\n",
    "print(\"File written\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "61189\n",
      "61189\n",
      "61189\n",
      "2\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "with open('../res/testing.csv', 'r') as train_stream:\n",
    "    count = 0\n",
    "    for i, line in enumerate(train_stream):\n",
    "        current_line = np.array(list(map(int, line.split(','))), dtype=np.int16)\n",
    "        print(len(current_line))\n",
    "        if i == 2:\n",
    "            break\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}