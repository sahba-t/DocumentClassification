{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load csv into dense matrix, then converts it to sparse matrix\n",
    "If \"res/sparse_training.data\" exists, **DO NOT** run cell as it will take awhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_matrix():\n",
    "    dense_matrix = np.zeros(shape=(21, 61188), dtype=np.int16)\n",
    "    with open('../res/training.csv', 'r') as train_stream:\n",
    "        for i, line in enumerate(train_stream):\n",
    "            line_int = np.array(list(map(int, line.split(','))), dtype=np.int16)\n",
    "            doc_label = line_int[-1]\n",
    "            dense_matrix[doc_label] += line_int[1:-1]\n",
    "            print(i)\n",
    "        sparse_training = sparse.csr_matrix(dense_matrix)\n",
    "        sparse.save_npz('../res/sparse_training.data',sparse_training)\n",
    "        \n",
    "# load_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 0)\t9\n",
      "  (1, 1)\t53\n",
      "  (1, 2)\t237\n",
      "  (1, 3)\t11\n",
      "  (1, 4)\t48\n",
      "  (1, 5)\t36\n",
      "  (1, 6)\t7\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t31\n",
      "  (1, 9)\t127\n",
      "  (1, 10)\t5\n",
      "  (1, 11)\t4336\n",
      "  (1, 12)\t19\n",
      "  (1, 13)\t24\n",
      "  (1, 14)\t46\n",
      "  (1, 15)\t597\n",
      "  (1, 16)\t267\n",
      "  (1, 17)\t12\n",
      "  (1, 18)\t9\n",
      "  (1, 19)\t15\n",
      "  (1, 20)\t2\n",
      "  (1, 21)\t3\n",
      "  (1, 22)\t2985\n",
      "  (1, 23)\t4\n",
      "  (1, 24)\t325\n",
      "  :\t:\n",
      "  (20, 61146)\t1\n",
      "  (20, 61147)\t1\n",
      "  (20, 61148)\t1\n",
      "  (20, 61149)\t2\n",
      "  (20, 61150)\t1\n",
      "  (20, 61151)\t1\n",
      "  (20, 61152)\t1\n",
      "  (20, 61153)\t1\n",
      "  (20, 61154)\t1\n",
      "  (20, 61169)\t2\n",
      "  (20, 61170)\t2\n",
      "  (20, 61171)\t2\n",
      "  (20, 61172)\t3\n",
      "  (20, 61173)\t4\n",
      "  (20, 61174)\t2\n",
      "  (20, 61175)\t3\n",
      "  (20, 61176)\t6\n",
      "  (20, 61177)\t1\n",
      "  (20, 61178)\t2\n",
      "  (20, 61179)\t2\n",
      "  (20, 61183)\t2\n",
      "  (20, 61184)\t2\n",
      "  (20, 61185)\t2\n",
      "  (20, 61186)\t2\n",
      "  (20, 61187)\t2\n"
     ]
    }
   ],
   "source": [
    "sparse_training_data = sparse.load_npz('../res/sparse_training.data.npz')\n",
    "# print(sparse_training_data[21, 3])  \n",
    "print(sparse_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating global vars and consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "UNIQUE_VOCAB = 61188\n",
    "TOTAL_VOCAB = sparse_training_data.sum()\n",
    "\n",
    "# the pxs used for mutual information\n",
    "pxs = np.zeros(61188, dtype=np.float)\n",
    "pxs_initialized=False\n",
    "\n",
    "set_list = [set() for x in range(0, 20)]\n",
    "class_row_dict = dict(zip(list(range(1, 21)), set_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Priors and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.04844692 0.05053315 0.05006954 0.05146036 0.04798331\n",
      " 0.05146036 0.05215577 0.05238758 0.05076495 0.04983774 0.05146036\n",
      " 0.04983774 0.04937413 0.05006954 0.05053315 0.05099675 0.05006954\n",
      " 0.05146036 0.04612888 0.04496987]\n"
     ]
    }
   ],
   "source": [
    "def count_priors():\n",
    "    word_id_ranges = list(range(1, 61189))\n",
    "    column_names =  ['doc_id'] + word_id_ranges + ['label']\n",
    "    p_counts = np.zeros(21, dtype=np.int16)\n",
    "    for i, data_chunk in enumerate(pd.read_csv('../res/training.csv', header=None, chunksize=200, names=column_names, usecols=['label'])): \n",
    "        for _, row in data_chunk.iterrows():\n",
    "            current_label = row['label']\n",
    "            class_row_dict[current_label].add(i)\n",
    "            i += 1\n",
    "    \n",
    "    for j in range(1, 21):\n",
    "        p_counts[j] = len(class_row_dict[j])\n",
    "    \n",
    "    return p_counts / p_counts.sum()\n",
    "\n",
    "prior_counts = count_priors()\n",
    "print(prior_counts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Px|Y used for both Question 6 and also Fast NB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Pxgy\n",
    "MUTUAL_BETA= 0.004\n",
    "pxgy = np.array(sparse_training_data.todense(), dtype=np.float)\n",
    "total_words = {}\n",
    "for i in range(1,21):\n",
    "    total_words[i] = pxgy[i].sum()\n",
    "    pxgy[i]  = (pxgy[i] + MUTUAL_BETA) / (total_words[i] + (MUTUAL_BETA * TOTAL_VOCAB))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Useful and lease useful words in the whole document. Using entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most usefull words!\n",
      "doves\n",
      "uxarch\n",
      "amphiboly\n",
      "unipress\n",
      "equivocation\n",
      "vaxstation\n",
      "stellix\n",
      "joanne\n",
      "newbauer\n",
      "lewart\n",
      "allbery\n",
      "wjh\n",
      "xgoodies\n",
      "pierces\n",
      "muscularly\n",
      "totty\n",
      "listname\n",
      "xew\n",
      "slit\n",
      "libwxm\n",
      "antiquitam\n",
      "crumenam\n",
      "univision\n",
      "adnt\n",
      "ncsuvm\n",
      "chirurgie\n",
      "boutilie\n",
      "xware\n",
      "netoprwa\n",
      "kck\n",
      "poop\n",
      "emits\n",
      "doleh\n",
      "conor\n",
      "giza\n",
      "shite\n",
      "sinkhole\n",
      "reification\n",
      "nuucp\n",
      "usrx\n",
      "baruch\n",
      "quinton\n",
      "favourating\n",
      "ehm\n",
      "moondog\n",
      "fileselector\n",
      "ryner\n",
      "tamm\n",
      "wergo\n",
      "foolishly\n",
      "xpic\n",
      "serverses\n",
      "navigations\n",
      "hangspace\n",
      "convenients\n",
      "respectedly\n",
      "divaev\n",
      "rusian\n",
      "fsck\n",
      "ifconfig\n",
      "mxterm\n",
      "ziemans\n",
      "woodlice\n",
      "moonsoft\n",
      "chiquita\n",
      "scherf\n",
      "multilist\n",
      "stna\n",
      "causa\n",
      "dgac\n",
      "michanek\n",
      "searle\n",
      "bouron\n",
      "connolly\n",
      "xcrichtext\n",
      "equivalently\n",
      "thyme\n",
      "ignorantium\n",
      "xmt\n",
      "dovetail\n",
      "ceilidh\n",
      "arguer\n",
      "multidrop\n",
      "smebsb\n",
      "wccreate\n",
      "wcchildren\n",
      "nssl\n",
      "unpacking\n",
      "conformant\n",
      "hotbed\n",
      "cwm\n",
      "pswm\n",
      "jbis\n",
      "carlotto\n",
      "championed\n",
      "hoagland\n",
      "facelike\n",
      "kerwin\n",
      "prather\n",
      "tycchow\n",
      "\n",
      "----------------------------------\n",
      "useless words:\n",
      "\n",
      "the\n",
      "to\n",
      "of\n",
      "and\n",
      "in\n",
      "is\n",
      "that\n",
      "it\n",
      "for\n",
      "you\n",
      "on\n",
      "this\n",
      "have\n",
      "be\n",
      "with\n",
      "not\n",
      "are\n",
      "or\n",
      "as\n",
      "if\n",
      "but\n",
      "edu\n",
      "they\n",
      "was\n",
      "can\n",
      "from\n",
      "at\n",
      "my\n",
      "by\n",
      "an\n",
      "there\n",
      "what\n",
      "all\n",
      "will\n",
      "would\n",
      "one\n",
      "do\n",
      "writes\n",
      "about\n",
      "com\n",
      "he\n",
      "so\n",
      "we\n",
      "has\n",
      "your\n",
      "no\n",
      "any\n",
      "me\n",
      "article\n",
      "some\n",
      "out\n",
      "like\n",
      "which\n",
      "don\n",
      "just\n",
      "who\n",
      "more\n",
      "up\n",
      "when\n",
      "get\n",
      "know\n",
      "only\n",
      "other\n",
      "their\n",
      "how\n",
      "people\n",
      "them\n",
      "were\n",
      "than\n",
      "had\n",
      "also\n",
      "think\n",
      "been\n",
      "use\n",
      "his\n",
      "does\n",
      "time\n",
      "then\n",
      "new\n",
      "good\n",
      "am\n",
      "these\n",
      "could\n",
      "should\n",
      "well\n",
      "may\n",
      "very\n",
      "now\n",
      "because\n",
      "even\n",
      "into\n",
      "apr\n",
      "much\n",
      "ve\n",
      "see\n",
      "system\n",
      "two\n",
      "way\n",
      "first\n",
      "make\n"
     ]
    }
   ],
   "source": [
    "xjointy = pxgy.copy()\n",
    "enthropy = np.zeros(61188)\n",
    "for i in range(1,21):\n",
    "    xjointy[i] *= prior_counts[i]\n",
    "for i in range(61188):\n",
    "    marginal_val = xjointy[:,i].sum()\n",
    "    enthropy[i] =  marginal_val * np.log2(marginal_val)\n",
    "sorted_indices = enthropy.argsort()\n",
    "vocab_df = pd.read_csv('../res/vocabulary.txt', sep='\\n', header=None, names=[\"word\"])\n",
    "print(\"Most usefull words!\")\n",
    "for x in sorted_indices[-100:][::-1]:\n",
    "    print(vocab_df.iloc[x]['word'])\n",
    "    \n",
    "print(\"\\n----------------------------------\")\n",
    "print(\"useless words:\\n\")\n",
    "for x in sorted_indices[:100]:\n",
    "    print(vocab_df.iloc[x]['word'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell calculates the mutual information per document class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignore warnings!\n",
      "pxs were calculated before. returning the result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "#calculates the p(X=xi) for all words\n",
    "def create_px():\n",
    "    global pxs_initialized\n",
    "    if not pxs_initialized:\n",
    "        print('generating all pxs this takes a few seconds')\n",
    "        for i in range(61188):\n",
    "            pxs[i] = sparse_training_data[:,i].sum()/TOTAL_VOCAB\n",
    "        pxs_initialized = True\n",
    "        print('pxs calculation complete!')    \n",
    "    else:\n",
    "        print('pxs were calculated before. returning the result')\n",
    "        \n",
    "# calculates the mutual information for all categories using the formula\n",
    "# MI = P(x,y) + log(p(x,y)/(p(x).p(y)))\n",
    "def calc_mutual():\n",
    "    create_px()\n",
    "    mutual_info = sparse_training_data.copy()\n",
    "    mutual_results = sparse.csr_matrix(np.zeros((21,61188)), dtype=np.float32)\n",
    "    non_zero_dict={}\n",
    "    for i in range(1,21):\n",
    "        row = mutual_info[i]\n",
    "        total_words = row.sum()\n",
    "        # print(row.shape)\n",
    "        non_zeros = mutual_info[i].nonzero()[1]\n",
    "        non_zero_dict[i] = non_zeros\n",
    "        # p(x|y)\n",
    "        pxgy = row.multiply(1/total_words)\n",
    "        pxy = pxgy.multiply(prior_counts[i])\n",
    "        pxgy[0,non_zeros] = np.log1p(pxgy[0,non_zeros].multiply(1/pxs[non_zeros]))\n",
    "        mutual_results[i, non_zeros] = pxy[0, non_zeros] + pxgy[0, non_zeros]\n",
    "    return mutual_results\n",
    "\n",
    "def print_mutual_results(mutual_results, vocab_df, doc_class_df, top_words=5):\n",
    "    results_dense =  mutual_results.todense()\n",
    "    best_words=[\"x\"] * top_words\n",
    "    worst_words = [\"x\"] * top_words\n",
    "    for i in range(1, 21):\n",
    "        row_dense = results_dense[i]\n",
    "        arg_maxes = row_dense.argsort()\n",
    "        print(doc_class_df.iloc[i-1]['doc_class'])\n",
    "        best_indices = arg_maxes[0,-top_words:][::-1]\n",
    "        worst_indices = arg_maxes[0,:top_words]\n",
    "        for ii in range(top_words):\n",
    "            best_index = best_indices[0,ii]\n",
    "            worst_index = worst_indices[0,ii]\n",
    "            best_words[ii] = vocab_df.iloc[best_index].word\n",
    "            worst_words[ii] = vocab_df.iloc[worst_index].word\n",
    "        print('most useful words')\n",
    "        print(best_words)\n",
    "        print('least useful words')\n",
    "        print(worst_words)\n",
    "\n",
    "print(\"ignore warnings!\")\n",
    "mutual_results = calc_mutual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The foloowing cell analyzes the mutual information calculations per document class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 alt.atheism\n",
      "most useful words\n",
      "['darice', 'buphy', 'bobbe', 'beauchaine', 'mozumder']\n",
      "least useful words\n",
      "['vijay', 'elaine', 'magnetometer', 'typ', 'jjj']\n",
      "2 comp.graphics\n",
      "most useful words\n",
      "['nfotis', 'radiosity', 'siggraph', 'pov', 'rayshade']\n",
      "least useful words\n",
      "['vijay', 'scuttle', 'definetly', 'rectifies', 'chops']\n",
      "3 comp.os.ms-windows.misc\n",
      "most useful words\n",
      "['ashok', 'louray', 'vgalogo', 'qw', 'rlk']\n",
      "least useful words\n",
      "['vijay', 'recommending', 'coords', 'microwaves', 'surveyors']\n",
      "4 comp.sys.ibm.pc.hardware\n",
      "most useful words\n",
      "['fasst', 'jmarttila', 'usma', 'rll', 'fdisk']\n",
      "least useful words\n",
      "['vijay', 'emisions', 'eaton', 'woodard', 'juhan']\n",
      "5 comp.sys.mac.hardware\n",
      "most useful words\n",
      "['adb', 'iivx', 'bmug', 'lciii', 'iisi']\n",
      "least useful words\n",
      "['vijay', 'demondulator', 'downconversion', 'ackermann', 'inverting']\n",
      "6 comp.windows.x\n",
      "most useful words\n",
      "['argv', 'oname', 'ndet', 'xdm', 'args']\n",
      "least useful words\n",
      "['vijay', 'rafters', 'entrances', 'nmw', 'furnace']\n",
      "7 misc.forsale\n",
      "most useful words\n",
      "['keown', 'punisher', 'liefeld', 'sabretooth', 'wolverine']\n",
      "least useful words\n",
      "['archive', 'poeldvere', 'jakobi', 'estonia', 'conductance']\n",
      "8 rec.autos\n",
      "most useful words\n",
      "['buick', 'infiniti', 'corvette', 'opel', 'geico']\n",
      "least useful words\n",
      "['etrbom', 'recchi', 'fedyk', 'galley', 'eklund']\n",
      "9 rec.motorcycles\n",
      "most useful words\n",
      "['blaine', 'motorcycling', 'cookson', 'countersteering', 'biker']\n",
      "least useful words\n",
      "['vijay', 'propranolol', 'psychogenic', 'lungs', 'inmet']\n",
      "10 rec.sport.baseball\n",
      "most useful words\n",
      "['hirschbeck', 'mattingly', 'rbi', 'alomar', 'pitcher']\n",
      "least useful words\n",
      "['vijay', 'inkjets', 'downloadable', 'cablevision', 'dissimilar']\n",
      "11 rec.sport.hockey\n",
      "most useful words\n",
      "['ahl', 'bruins', 'nyr', 'leafs', 'nhl']\n",
      "least useful words\n",
      "['vijay', 'chiggers', 'affliction', 'gainesville', 'chigger']\n",
      "12 sci.crypt\n",
      "most useful words\n",
      "['cryptographic', 'plaintext', 'denning', 'crypt', 'ripem']\n",
      "least useful words\n",
      "['vijay', 'psychiatric', 'enacted', 'neurology', 'neurologist']\n",
      "13 sci.electronics\n",
      "most useful words\n",
      "['deaddio', 'capacitors', 'bubblejets', 'cmkrnl', 'adcom']\n",
      "least useful words\n",
      "['archive', 'nfpa', 'schwan', 'vacate', 'inspectors']\n",
      "14 sci.med\n",
      "most useful words\n",
      "['antibiotic', 'antibiotics', 'hicnet', 'dyer', 'candida']\n",
      "least useful words\n",
      "['vijay', 'cryptophone', 'cylink', 'laissez', 'blames']\n",
      "15 sci.space\n",
      "most useful words\n",
      "['ssto', 'prb', 'nsmca', 'baalke', 'orbiter']\n",
      "least useful words\n",
      "['vijay', 'keyspaces', 'disctribution', 'usenetters', 'lawfulness']\n",
      "16 soc.religion.christian\n",
      "most useful words\n",
      "['ahmadiyya', 'arsenokoitai', 'jayne', 'clh', 'athos']\n",
      "least useful words\n",
      "['archive', 'casserole', 'tlu', 'gic', 'ethridge']\n",
      "17 talk.politics.guns\n",
      "most useful words\n",
      "['shotguns', 'rutledge', 'thomasp', 'kratz', 'homicides']\n",
      "least useful words\n",
      "['vijay', 'slogans', 'shiriff', 'speculates', 'selectively']\n",
      "18 talk.politics.mideast\n",
      "most useful words\n",
      "['gaza', 'serdar', 'israelis', 'argic', 'istanbul']\n",
      "least useful words\n",
      "['vijay', 'redpoll', 'depew', 'corrosive', 'eichener']\n",
      "19 talk.politics.misc\n",
      "most useful words\n",
      "['schnopia', 'locutus', 'hendricks', 'steveh', 'stephanopoulos']\n",
      "least useful words\n",
      "['archive', 'behavioural', 'jhan', 'dgbt', 'vunerable']\n",
      "20 talk.religion.misc\n",
      "most useful words\n",
      "['caligiuri', 'mcconkie', 'zoroastrians', 'zarathushtra', 'royalroads']\n",
      "least useful words\n",
      "['vijay', 'beersheba', 'pasture', 'derby', 'sscl']\n"
     ]
    }
   ],
   "source": [
    "vocab_df = pd.read_csv('../res/vocabulary.txt', sep='\\n', header=None, names=[\"word\"])\n",
    "doc_class_df = pd.read_csv('../res/newsgrouplabels.txt', sep='\\n', header=None, names=[\"doc_class\"])\n",
    "#loading\n",
    "print_mutual_results(mutual_results, vocab_df, doc_class_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "formula from the proj2 PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def multinomial_naive_bayes(row, row_total_words, alpha, debug_prints = False)->int:\n",
    "    if debug_prints: print(row_total_words, row)\n",
    "        \n",
    "    k = alpha - 1\n",
    "    map_denom = np.zeros(21, dtype = np.float64)\n",
    "    for i in range(1, 21):\n",
    "        map_denom[i] = sparse_training_data[i].sum() + (k * TOTAL_VOCAB)\n",
    "        \n",
    "    max_prob_class = [float('-inf'), -1]\n",
    "    for doc_label in range(1, 21):\n",
    "        running_sum = 0\n",
    "        for word_i, num_words_at_i in row:\n",
    "            running_sum += np.log2((sparse_training_data[doc_label, word_i] + k)/map_denom[doc_label])\n",
    "        posterior = running_sum + np.log2(prior_counts[doc_label])\n",
    "        if posterior > max_prob_class[0]:\n",
    "            max_prob_class[0] = posterior\n",
    "            max_prob_class[1] = doc_label\n",
    "\n",
    "    if debug_prints: print(max_prob_class)\n",
    "    return max_prob_class[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bernoulli_naive_bayes(row, row_total_words, alpha, debug_prints = False)->int:\n",
    "    if debug_prints: print(row_total_words, row)\n",
    "    \n",
    "    # num_zeros = UNIQUE_VOCAB - len(row)\n",
    "    max_prob_class = [float('-inf'), -1]\n",
    "    # k = alpha - 1\n",
    "\n",
    "    # prob_not_appearing = (0+k) / denom\n",
    "    # # prob_of_doc = 1/20  # Not needed because all will be multiplied by it\n",
    "    # unique_words  = 0\n",
    "    for doc_label in range(1, 21):\n",
    "        running_sum = 0\n",
    "        # for word_i, num_words_at_i in row:\n",
    "        #     pass\n",
    "        \n",
    "        posterior = running_sum\n",
    "        if posterior > max_prob_class[0]:\n",
    "            max_prob_class[0] = posterior\n",
    "            max_prob_class[1] = doc_label\n",
    "\n",
    "    if debug_prints: print(max_prob_class)\n",
    "    return max_prob_class[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_training(bayes_function, alpha):\n",
    "    sparse_matrix = sparse.load_npz('../res/nb_training_data.npz')\n",
    "    training_data_coo = sparse_matrix.tocoo()\n",
    "    \n",
    "    row_total_words = training_data_coo.A.sum(axis=1)\n",
    "    correct = 0\n",
    "    row = []\n",
    "    row_i = 0\n",
    "    for row_i, word_i, val in zip(training_data_coo.row, training_data_coo.col, training_data_coo.data):\n",
    "        if word_i != 61188:\n",
    "            \n",
    "            row.append((word_i, val))\n",
    "        else:\n",
    "            classification = bayes_function(row, row_total_words[row_i] - val, alpha=alpha)\n",
    "            correct += 1 if classification == val else 0\n",
    "            row.clear()\n",
    "            if not row_i % 500:\n",
    "                print('At row:', row_i)\n",
    "    print(\"Finished\")\n",
    "    print(f\"accuracy: {(correct / row_i) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_and_write(bayes_function, write_path, alpha):\n",
    "    sparse_matrix = sparse.load_npz('../res/nb_testing_data.npz')\n",
    "    testing_data_coo = sparse_matrix.tocoo()\n",
    "    with open(write_path, 'w') as out_stream:\n",
    "        out_stream.write(\"id,class\\n\")\n",
    "    \n",
    "        row_total_words = testing_data_coo.A.sum(axis=1)\n",
    "        row_offset = 12000\n",
    "        current_row = 12000\n",
    "        row = []\n",
    "        for row_i, word_i, num_words_at_i in zip(testing_data_coo.row + row_offset, testing_data_coo.col, testing_data_coo.data):\n",
    "            if row_i == current_row:\n",
    "                row.append((word_i, num_words_at_i))\n",
    "            else:\n",
    "                predicted_label = bayes_function(row, row_total_words[row_i - row_offset], alpha=alpha)\n",
    "                out_stream.write(f'{row_i},{predicted_label}\\n')\n",
    "                row.clear()\n",
    "                current_row = row_i\n",
    "                if not current_row % 500:\n",
    "                    print('At row:', current_row)\n",
    "    print(\"File written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "- multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b = .004 # BETA: Tried -> 1/TOTAL_VOCAB, .00001, .0001, .001, .01, .1, 1\n",
    "a = 1 + b  # ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "print('beta:', b)\n",
    "test_training(multinomial_naive_bayes, alpha=a)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60, 'minutes.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "print('beta:', b)\n",
    "file_path = '../results/multinomial_NB_b' + str(b)[(2 if b < 1 else 0) :] +'_results.csv'\n",
    "test_and_write(multinomial_naive_bayes, file_path, alpha=a)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60, 'minutes.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "print('beta:', b)\n",
    "test_training(bernoulli_naive_bayes, alpha=a)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60, 'minutes.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "start = timeit.default_timer()\n",
    "\n",
    "print('beta:', b)\n",
    "file_path = '../results/bernoulli_NB_b' + str(b)[(2 if b < 1 else 0) :] +'_results.csv'\n",
    "test_and_write(bernoulli_naive_bayes, file_path, alpha=a)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60, 'minutes.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de3hU9b3v8fc3CSEhxHghsLmKWBSpNyAiapGbchPIGru91t29W57tptU+7WnrU9vunrOr7dk9XtrdjfZU9m639TlbPRYNQRAQQUCtugkevCDa4pWISKBeuCRAku/5YwY6hEkygaxZM8nn9TzzOGvNWjOfmQfXJ+u3Zs0yd0dERKSlvKgDiIhIdlJBiIhISioIERFJSQUhIiIpqSBERCQlFYSIiKRUEHWAztSnTx8fOnRo1DFERHLGhg0bdrp7earHulRBDB06lJqamqhjiIjkDDN7r7XHNMQkIiIpqSBERCQlFYSIiKSkghARkZRUECIikpIKQkREUupSX3MVkfTs3r2bDz/8kO3bt1NfX8/AgQMZNGgQZWVlmFnU8SRLqCBEugh3Z9euXXz44YeHb9u3bz9i+tBt7969KZ+jpKSEwYMHM2jQoCNuyfNOOukklUg3oYIQyXKNjY189NFHR23kW278t2/fzsGDB49av3fv3vTv35/+/fszZswY/uqv/urwdP/+/SkuLmbbtm1s3bqV2traw7ennnqKbdu20dzcfMTz9erVq9UCueiiizjllFMy9dFIyFQQIiFqbGxk7969R9z27NnT5ry6urojNv51dXWkuvJjnz59Dm/sR4wYccRGv3///ocf692793Hl3759+xHFkVwka9as4YMPPqCpqQmAAQMGsGrVKkaMGHHMrynZw7rSJUcrKipcP7UhHdXc3My+fftSbrzbm25vmf3793coS1FREaecckrKDX3yrV+/fhQWFob0iXRMU1MTH330EZs2beKGG24A4KmnnuKcc86JOJmkw8w2uHtFysdUEJKr3J09e/awY8cO6urqjrh99tlnaW/c6+vrO/S6hYWFlJSUUFJSQu/evQ/fTzWdzjKHpnv16kV+fn5In1ZmvPHGG0yZMoWGhgZWrlzJ6NGjo44k7WirIDTEJFnD3dm9e/fhjXzyhr9lCRyabu0v9Pz8/JQb4pNOOolBgwYd88a9pKSEHj16ZPiTyR0jRoxg3bp1TJ48mcmTJ7N8+XLGjRsXdSw5RioICc2hDX46G/pDt9Y2+CUlJZSXl1NeXk7//v0599xzD0/37dv3qPvFxcX6pk1ETj/9dJ555hkmT57M5ZdfztKlS7n00kujjiXHQAUhaXN3Pvvss3Y39Mn3Dxw4kPK5SkpKDm/MBwwYwHnnnZdyQ3/o1qtXrwy/WzkeQ4YMYd26dUyZMoXp06dTXV3N5ZdfHnUs6SAVRDfm7nz66adp/XW/Y8cOdu7c2eoGv3fv3oc35oMGDWLUqFEpN/TJf+FL1zZgwADWrl3L5ZdfzuzZs1m4cCGzZs2KOpZ0gAqiCzm0wU/3r/u6urqU35uH+Ab/0MZ80KBBjB49OuWG/tBNG3xJpW/fvjz99NNMnTqVK6+8kocffpgrr7wy6liSJhVEFnN3Pvnkk7QP2u7cubPVDX5paenhDfuQIUMYM2ZMq+P35eXlFBUVZfjdSld18skns2rVKmbMmMHVV1/NAw88wPXXXx91LEmDCiKDDm3wO3LQtrGxMeVzlZaWHt6gDxkyhIqKijb/wtcGX6JUVlbGihUrmD17NjfccAMNDQ189atfjTqWtEMFcRyam5sP/4WfzkZ/586drW7wTzjhhMMb9qFDh3LBBRe0+hd+nz59tMGXnFNaWsoTTzxBLBZj7ty5NDQ08PWvfz3qWNIGFUSS5uZmPv744w4dtD30EwMtlZWVHd6wn3baaVx44YWt/nVfXl5Oz549M/xuRTKvV69eVFdXc/XVV3PTTTfR0NDAt7/97ahjSSu6fUG4O6NHj+bDDz9Ma4Pft29fhg0bxoUXXtjqN3T69OmjDb5IK4qKili4cCFf+tKX+M53vkN9fT0//OEPo44lKXT7gjAzzjnnnMNDOqmGdfr06ZM1v3sj0hUUFhby0EMPUVRUxD/+4z/S0NDAbbfdppMbs0yoBWFm04FfAvnAv7v7z1o8Xgb8H2BIIstd7v4ficfeBXYDTUBja78V0hkeeOCBsJ5aRFpRUFDA/fffT1FRET/5yU+or6/nzjvvVElkkdAKwszygXuBy4FaYL2ZLXb315MWuwl43d1nm1k58KaZ/ae7Hzoba5K77wwro4hEKz8/n/vuu4+ePXty9913U19fz/z588nL09WQs0GYexBjgS3u/jaAmT0MVALJBeFAqcX/ZOgN/BlI/TUfEemS8vLymD9/PsXFxdx11100NDSwYMGCnP9l264gzIIYCGxNmq4FLmyxzD3AYmAbUApc4+6HLl/lwJNm5sB97r4g1YuY2Y3AjRD//RcRyT1mxh133EFxcTG33347DQ0N/O53v6OgoNsfJo1UmJ9+qoHElhefmAZsBCYDpwMrzewZd/8MuMTdt5lZ38T8N9x93VFPGC+OBRC/HkSnvgMRyRgz47bbbqO4uJgf/OAH7N+/nwcffFBfEIlQmAN9tcDgpOlBxPcUkn0FeMzjtgDvACMA3H1b4r87gCriQ1Yi0sV9//vf5xe/+AWPPvooX/ziF2loaIg6UrcVZkGsB4ab2WlmVghcS3w4Kdn7wBQAM+sHnAm8bWYlZlaamF8CTAVeCzGriGSRb33rW/zqV79iyZIlzJkzh3379kUdqVsKbYjJ3RvN7GZgBfGvuf7W3TeZ2bzE478GbgfuN7NXiQ9Jfc/dd5rZMKAq8XW3AuBBd18eVlYRyT5f+9rXKCoqYu7cucycOZPHH3+c0tLSqGN1K7omtYhktYceeoi/+Zu/4YILLmDZsmWceOKJUUfqUtq6JrW+bCwiWe26667j97//PRs2bGDKlCns2rUr6kjdhgpCRLJeLBZj0aJFbNq0iUmTJvHRRx9FHalbUEGISE6YOXMmS5YsYcuWLUycOJEPPvgg6khdngpCRHLGZZddxooVK6itrWXChAm89957UUfq0lQQIpJTxo8fz1NPPcWuXbu49NJLeeutt6KO1GWpIEQk51x44YWsXr2avXv3Mn78eN54442oI3VJKggRyUmjRo1izZo1NDc3M2HCBF599dWoI3U5KggRyVlnn302a9eupUePHkycOJENGzZEHalLUUGISE4788wzWbduHaWlpUyZMoXnn38+6khdhgpCRHLesGHDWLduHeXl5UydOpV164764Wc5BioIEekShgwZwrp16xg8eDDTp09n5cqVUUfKeSoIEeky+vfvz5o1axg+fDizZs1iyZIlUUfKaSoIEelS+vbty9NPP825555LLBbj0UcfjTpSzlJBiEiXc/LJJ/PUU08xduxYrrnmGh588MGoI+UkFYSIdEllZWWsWLGC8ePHc8MNN/Db3/426kg5RwUhIl1W7969eeKJJ5g6dSpz587l3nvvjTpSTlFBiEiXVlxcTHV1NXPmzOHmm2/m7rvvjjpSzlBBiEiX17NnTxYuXMhVV13Fd7/7XX7yk59EHSknhHZNahGRbNKjRw8efPBBioqK+NGPfkRDQwO33347ZhZ1tKylghCRbqOgoID777+fnj178tOf/pT6+nruuusulUQrVBAi0q3k5eVx3333UVxczM9//nMaGhqYP38+eXkacW9JBSEi3U5eXh6//OUvKS4u5o477qChoYEFCxaQn58fdbSsooIQkW7JzPjZz35GcXExP/7xjykqKtLXYFvQPpWIdFtmxj/90z8xd+5cfvOb37B79+6oI2UVFYSIdHtf/vKX2b9/P8uXL486SlYJtSDMbLqZvWlmW8zs1hSPl5nZ42b2spltMrOvpLuuiEhnufjii+nTpw+LFi2KOkpWCa0gzCwfuBeYAYwErjOzkS0Wuwl43d3PAyYCd5tZYZrrioh0ioKCAmbPns3SpUs5cOBA1HGyRph7EGOBLe7+trsfAB4GKlss40Cpxb+E3Bv4M9CY5roiIp0mFovx6aefsmbNmqijZI0wC2IgsDVpujYxL9k9wFnANuBV4Jvu3pzmugCY2Y1mVmNmNXV1dZ2VXUS6mcsuu4xevXppmClJmAWR6tREbzE9DdgIDADOB+4xsxPSXDc+032Bu1e4e0V5efnx5BWRbqy4uJjp06dTXV1Nc3Nz1HGyQpgFUQsMTpoeRHxPIdlXgMc8bgvwDjAizXVFRDpVEARs27aN9evXRx0lK4RZEOuB4WZ2mpkVAtcCi1ss8z4wBcDM+gFnAm+nua6ISKeaNWsW+fn5GmZKCK0g3L0RuBlYAWwGHnH3TWY2z8zmJRa7HbjYzF4FVgHfc/edra0bVlYREYCTTjqJiRMnqiASQv2pDXd/AniixbxfJ93fBkxNd10RkbAFQcA3vvEN3njjDUaMGBF1nEjpTGoRkSSVlfFv1GsvQgUhInKEwYMHU1FRoYJABSEicpQgCHjxxRfZtq17f3lSBSEi0kIQBABUV1dHnCRaKggRkRZGjhzJ8OHDu/0wkwpCRKQFMyMIAlavXs0nn3wSdZzIqCBERFIIgoDGxkaWLVsWdZTIqCBERFIYN24c/fr1o6qqKuookVFBiIikkJeXR2VlJcuWLaOhoSHqOJFQQYiItCIIAvbs2cPq1aujjhIJFYSISCsmT55MaWlptx1mUkGIiLSiZ8+ezJw5k8WLF9PU1BR1nIxTQYiItCEIAnbs2MELL7wQdZSMU0GIiLRhxowZ9OjRo1ueNKeCEBFpQ1lZGVOmTKGqqgr3lFc+7rJUECIi7QiCgLfeeotNm7rXdctUECIi7ZgzZw7Q/a4RoYIQEWlH//79GTduXLf7uqsKQkQkDbFYjJdeeon3338/6igZo4IQEUlDd7xGRLsFYWazzExFIiLd2hlnnMFZZ53VrY5DpLPhvxb4k5ndYWZnhR1IRCRbxWIx1q5dy65du6KOkhHtFoS73wCMAt4C/sPMnjezG82sNPR0IiJZJAgCmpqaWLp0adRRMiKtoSN3/wx4FHgY6A/EgJfM7BshZhMRySpjxoxh4MCB3WaYKZ1jELPNrApYDfQAxrr7DOA84Lsh5xMRyRp5eXkEQcDy5cvZt29f1HFCl84exFXAL9z9XHe/0913ALj7PuCroaYTEckyQRBQX1/PypUro44SunQK4n8A/3VowsyKzWwogLuvamtFM5tuZm+a2RYzuzXF47eY2cbE7TUzazKzkxOPvWtmryYeq+nImxIRCcuECRMoKyvrFsNM6RTE74HmpOmmxLw2mVk+cC8wAxgJXGdmI5OXSeyRnO/u5wPfB9a6+5+TFpmUeLwijZwiIqHr0aMHs2bN4vHHH6exsTHqOKFKpyAK3P3AoYnE/cI01hsLbHH3txPrPAxUtrH8dcBDaTyviEikYrEYu3bt4tlnn406SqjSKYg6M5tzaMLMKoGdaaw3ENiaNF2bmHcUM+sFTCf+TalDHHjSzDaY2Y2tvUjiK7c1ZlZTV1eXRiwRkeMzbdo0evbs2eWHmdIpiHnAD8zsfTPbCnwP+Ic01rMU81r7MfXZwHMthpcucffRxIeobjKzS1Ot6O4L3L3C3SvKy8vTiCUicnx69+7N5ZdfzqJFi7r0NSLSOVHuLXcfR/w4wkh3v9jdt6Tx3LXA4KTpQcC2Vpa9lhbDS+6+LfHfHUAV8SErEZGsEIvFeO+999i4cWPUUUJTkM5CZnYF8HmgyCy+Y+Dut7Wz2npguJmdBnxAvASuT/HcZcAE4IakeSVAnrvvTtyfCrT3eiIiGTN79mzy8vJYtGgRo0aNijpOKNI5Ue7XwDXAN4gPG10FnNreeu7eCNwMrAA2A4+4+yYzm2dm85IWjQFPuvvepHn9gGfN7GXiX7Fd6u7L03xPIiKhKy8v55JLLunSxyGsvfEzM3vF3c9N+m9v4DF3n5qZiOmrqKjwmhqdMiEimfHzn/+c73znO7z11lsMGzYs6jjHxMw2tHYqQToHqRsS/91nZgOAg8BpnRVORCRXHbpGRFfdi0inIB43sxOBO4GXgHfR+QoiIgwbNoxzzz23exZE4kJBq9z9E3d/lPixhxHu/t8zkk5EJMsFQcBzzz3Hjh07oo7S6dosCHdvBu5Omt7v7p+GnkpEJEfEYjGam5t5/PHHo47S6dIZYnrSzL5oh77fKiIih5133nmceuqpXXKYKZ2C+DbxH+fbb2afmdluM/ss5FwiIjnBzAiCgJUrV7Jnz56o43SqdM6kLnX3PHcvdPcTEtMnZCKciEguCIKA/fv3s3x51zpdq90zqdv4DaR1nR9HRCT3fOELX+CUU05h0aJF/PVf/3XUcTpNOj+1cUvS/SLiv4m0AZgcSiIRkRxTUFDA7Nmzqaqq4uDBg/To0SPqSJ0inSGm2Um3y4GzgY/CjyYikjuCIODTTz9l7dq1UUfpNOkcpG6plnhJiIhIwtSpU+nVqxdVVVVRR+k06RyDmM9fruOQB5wPvBxmKBGRXFNcXMy0adOorq5m/vz55OUdy9/f2SWdd1BD/JjDBuB54HvufkPbq4iIdD9BEPDBBx+wYcOGqKN0inQOUi8EGty9CcDM8s2sl7vvCzeaiEhumTVrFvn5+VRVVXHBBRdEHee4pbMHsQooTpouBp4KJ46ISO46+eSTmTBhQpc5qzqdgihy98OnBybu9wovkohI7gqCgM2bN/Pmm29GHeW4pVMQe81s9KEJMxsD1IcXSUQkd1VWVgJQXV0dcZLjl05BfAv4vZk9Y2bPAP+X+KVERUSkhSFDhjBmzJgu8XXXdE6UWw+MAL4GfB04y927xiF6EZEQBEHACy+8wIcffhh1lOPSbkGY2U1Aibu/5u6vAr3N7OvhRxMRyU2HLkW6ePHiiJMcn3SGmP7e3T85NOHuHwN/H14kEZHc9vnPf57TTz8954eZ0imIvOSLBZlZPlAYXiQRkdxmZsRiMVavXs2nn+buRTjTKYgVwCNmNsXMJgMPAcvCjSUiktuCIODgwYMsW5a7m8t0CuJ7xE+W+xpwE/AKR544JyIiLYwbN46+ffvm9Elz6XyLqRl4AXgbqACmAJtDziUiktPy8/OprKzkiSeeYP/+/VHHOSatFoSZnWFm/93MNgP3AFsB3H2Su9+TzpOb2XQze9PMtpjZrSkev8XMNiZur5lZk5mdnM66IiLZLggCdu/ezerVq6OOckza2oN4g/jewmx3/4K7zwea0n3ixMHse4EZwEjgOjMbmbyMu9/p7ue7+/nA94G17v7ndNYVEcl2kydPpnfv3jk7zNRWQXwR2A48bWb/ZmZTAGtj+ZbGAlvc/W13PwA8DFS2sfx1xA+AH8u6IiJZp6ioiJkzZ1JdXU1TU9p/X2eNVgvC3avc/RriZ1GvAf4b0M/M/reZTU3juQeSGJZKqE3MO4qZ9QKmA48ew7o3mlmNmdXU1dWlEUtEJHOCIOCjjz7ixRdfjDpKh6VzkHqvu/+nu88CBgEbgXSOCaTa2/AU8wBmA8+5+587uq67L3D3CnevKC8vTyOWiEjmzJw5kx49euTkMFOHronn7n929/vcfXIai9cCg5OmBwHbWln2Wv4yvNTRdUVEslZZWRmTJk2iqqoK99b+Rs5OYV40dT0w3MxOM7NC4iVw1A+TmFkZMAGo7ui6IiK5IBaLsWXLFl5//fWoo3RIaAXh7o3EfxZ8BfHzJh5x901mNs/M5iUtGgOedPe97a0bVlYRkTDNmTMHIOeGmSzXdnnaUlFR4TU1NVHHEBE5yrhx42hqamL9+vVRRzmCmW1w94pUj4U5xCQiIgmxWIyamhq2bt3a/sJZQgUhIpIBh64RkUuXIlVBiIhkwJlnnsmIESNy6jiECkJEJEOCIGDNmjV8/PHHUUdJiwpCRCRDYrEYTU1NLFmyJOooaVFBiIhkSEVFBQMGDMiZYSYVhIhIhuTl5VFZWcny5cupr6+POk67VBAiIhkUi8XYt28fK1eujDpKu1QQIiIZNGHCBMrKynJimEkFISKSQYWFhVxxxRUsXryYxsbGqOO0SQUhIpJhQRCwa9cunnvuuaijtEkFISKSYdOnT6dnz55ZP8ykghARybDS0lIuu+wyFi1alNXXiFBBiIhEIAgC3n33XV555ZWoo7RKBSEiEoE5c+ZgZlRVVUUdpVUqCBGRCPTt25dLLrkkq49DqCBERCISBAEvv/wy77zzTtRRUlJBiIhE5NA1IrJ1L0IFISISkdNPP51zzjlHBSEiIkcLgoBnn32Wurq6qKMcRQUhIhKhIAhobm7OymtEqCBERCI0atQohgwZkpVfd1VBiIhEyMwIgoAnn3ySPXv2RB3nCCoIEZGIBUHA/v37efLJJ6OOcgQVhIhIxMaPH8/JJ5+cdcNMoRaEmU03szfNbIuZ3drKMhPNbKOZbTKztUnz3zWzVxOP1YSZU0QkSgUFBcyePZslS5Zw8ODBqOMcFlpBmFk+cC8wAxgJXGdmI1sscyLwK2COu38euKrF00xy9/PdvSKsnCIi2SAIAj755BPWrVsXdZTDwtyDGAtscfe33f0A8DBQ2WKZ64HH3P19AHffEWIeEZGsNXXqVIqLi7PqpLkwC2IgsDVpujYxL9kZwElmtsbMNpjZl5Mec+DJxPwbQ8wpIhK5Xr16MW3atKy6RkSYBWEp5rV81wXAGOAKYBrwIzM7I/HYJe4+mvgQ1U1mdmnKFzG70cxqzKwmG89EFBFJVxAE1NbWsmHDhqijAOEWRC0wOGl6ELAtxTLL3X2vu+8E1gHnAbj7tsR/dwBVxIesjuLuC9y9wt0rysvLO/ktiIhkzqxZs8jPz8+aYaYwC2I9MNzMTjOzQuBaYHGLZaqB8WZWYGa9gAuBzWZWYmalAGZWAkwFXgsxq4hI5E455RQuvfTSrPm6a2gF4e6NwM3ACmAz8Ii7bzKzeWY2L7HMZmA58ArwX8C/u/trQD/gWTN7OTF/qbsvDyuriEi2CIKA119/nT/+8Y9RR8Gy5WBIZ6ioqPCaGp0yISK567333mPo0KHccccd3HLLLaG/npltaO1UAp1JLSKSRU499VRGjRqVFcchVBAiIlkmFovx/PPPs3379khzqCBERLJMEAS4O4sXt/xeT2apIEREsszZZ5/NsGHDIh9mUkGIiGQZMyMWi7Fq1So+++yzyHKoIEREslAQBBw4cIBly5ZFlkEFISKShS666CLKy8sjHWZSQYiIZKH8/HzmzJnD0qVL2b9/fyQZVBAiIlkqFouxe/dunn766UheXwUhIpKlpkyZQklJSWTDTCoIEZEsVVRUxIwZM6iurqa5uTnjr6+CEBHJYrFYjO3bt/Piiy9m/LVVECIiWWzmzJkUFBREMsykghARyWInnngikyZNoqqqKuOXIlVBiIhkuSAI+NOf/sTmzZsz+roqCBGRLFdZWQmQ8WEmFYSISJYbOHAgY8eOVUGIiMjRgiBg/fr11NbWZuw1VRAiIjkgFosBUF1dnbHXVEGIiOSAESNGcOaZZ2Z0mEkFISKSI4IgYM2aNXz88ccZeT0VhIhIjgiCgMbGRpYuXZqR11NBiIjkiLFjx9K/f/+MDTOpIEREckReXh6VlZUsX76c+vr68F8v9FcQEZFOEwQBe/fuZdWqVaG/lgpCRCSHTJo0iRNOOIGqqqrQXyvUgjCz6Wb2ppltMbNbW1lmopltNLNNZra2I+uKiHQ3hYWFXHHFFSxevJimpqZQXyu0gjCzfOBeYAYwErjOzEa2WOZE4FfAHHf/PHBVuuuKiHRXQRCwc+dO/vCHP4T6OmHuQYwFtrj72+5+AHgYqGyxzPXAY+7+PoC77+jAuiIi3dKMGTMoLCwMfZgpzIIYCGxNmq5NzEt2BnCSma0xsw1m9uUOrAuAmd1oZjVmVlNXV9dJ0UVEsldpaSmXXXYZixYtCvUaEWEWhKWY1/KdFABjgCuAacCPzOyMNNeNz3Rf4O4V7l5RXl5+PHlFRHJGEAS88847vPrqq6G9RpgFUQsMTpoeBGxLscxyd9/r7juBdcB5aa4rItJtzZkzBzML9aS5MAtiPTDczE4zs0LgWmBxi2WqgfFmVmBmvYALgc1prisi0m3169ePiy++ONTjEKEVhLs3AjcDK4hv9B9x901mNs/M5iWW2QwsB14B/gv4d3d/rbV1w8oqIpKLgiBg48aNvPvuu6E8v2X6Ithhqqio8JqamqhjiIhkxJYtWxg+fDj/8i//wje/+c1jeg4z2+DuFake05nUIiI56nOf+xxnn312aMNMKggRkRwWi8VoaGjg4MGDnf7cGmISEclhzc3N5OUd+9/6GmISEemijqcc2n3u0J5ZRERymgpCRERSUkGIiEhKKggREUlJBSEiIimpIEREJCUVhIiIpNSlTpQzszrgvaRZZcCnbazS2uMt57c1nep+8rw+wM404qebrb3HU81vL29r95U9/cc7kj15Oheyt5zuCtmT74edPd2cqeZlIvuJ7p76Yjru3mVvwIJjebzl/LamU91vMa8myuzp5G3jfSh7CNnb+LeSldnbyJmz2VO9j7Cyp5szG7N39SGmx4/x8Zbz25pOdb+9101HZ2VvOa+j94+Fsh89r7V/Q7mQveV0V8iefD/s7K0tk/XZu9QQUzYysxpv5XdOsp2yR0PZo6HsR+vqexDZYEHUAY6DskdD2aOh7C1oD0JERFLSHoSIiKSkghARkZRUECIikpIKIiJmlmdmPzWz+Wb2t1Hn6Qgzm2hmz5jZr81sYtR5OsrMSsxsg5nNijpLR5nZWYnPfaGZfS3qPB1hZoGZ/ZuZVZvZ1KjzdISZDTOz35jZwqizpCPxb/x3ic/7S8f6PCqIY2BmvzWzHWb2Wov5083sTTPbYma3tvM0lcBA4CBQG1bWljopuwN7gCJyLzvA94BHwknZus7I7+6b3X0ecDWQsa9kdlL2Re7+98DfAdeEGPcInZT9bXefG27StnXwfVwJLEx83nOO+UWP5ey77n4DLgVGA68lzcsH3gKGAYXAy8BI4BxgSYtbX+BW4B8S6y7Msex5ifX6Af+ZY9kvA6k7ctYAAAOLSURBVK4lvpGalWv/bhLrzAH+AFyfa9kT690NjM7R7Bn7f/U438f3gfMTyzx4rK9ZgHSYu68zs6EtZo8Ftrj72wBm9jBQ6e7/DBw1lGFmtcCBxGRTeGmP1BnZk3wM9AwjZyqd9LlPAkqI/09Ub2ZPuHtzqMETOuuzd/fFwGIzWwo8GF7iI16zMz57A34GLHP3l8JN/Bed/G8+Mh15H8T37AcBGzmOkSIVROcZCGxNmq4FLmxj+ceA+WY2HlgXZrA0dCi7mV0JTANOBO4JN1q7OpTd3X8IYGZ/B+zMVDm0oaOf/UTiwwc9gSdCTda+jv6b/wbxPbgyM/ucu/86zHDt6OjnfgrwU2CUmX0/USTZoLX38a/APWZ2BcfxcxwqiM5jKea1ehaiu+8DIh3TTNLR7I8RL7hs0KHshxdwv7/zoxyTjn72a4A1YYXpoI5m/1fiG65s0NHsu4B54cU5Zinfh7vvBb5yvE+ug9SdpxYYnDQ9CNgWUZaOUvbo5HJ+ZY9eqO9DBdF51gPDzew0MyskfiB0ccSZ0qXs0cnl/MoevXDfR1RH5HP5BjwEfMhfvqI6NzF/JvBH4t8q+GHUOZU9u265nF/Zo79F8T70Y30iIpKShphERCQlFYSIiKSkghARkZRUECIikpIKQkREUlJBiIhISioIkWNkZk1mttHMXjazl8zs4naWP9HMvp6pfCLHS+dBiBwjM9vj7r0T96cBP3D3CW0sPxRY4u5nZyahyPHRHoRI5ziB+M+fA2Bmt5jZejN7xcx+nJj9M+D0xF7HnWbW28xWJfY+XjWzykiSi7RCv+YqcuyKzWwj8Svr9QcmA1j8cprDif9WvxG/dsOlxC8Sdba7n59YrgCIuftnZtYHeMHMFrt26yVLqCBEjl190sb+IuABMzsbmJq4/b/Ecr2JF8b7LdY34H8myqOZ+G/79wO2ZyC7SLtUECKdwN2fT+wFlBPf8P+zu9+XvEyKq4F9KbH8GHc/aGbvEt8bEckKOgYh0gnMbATx6wPvAlYAXzWzQwewB5pZX2A3UJq0WhmwI1EOk4BTMxxbpE3agxA5doeOQUB8r+Fv3b0JeNLMzgKej1+GmT3ADe7+lpk9Z2avAcuA/wU8bmY1xK8d/Ebm34JI6/Q1VxERSUlDTCIikpIKQkREUlJBiIhISioIERFJSQUhIiIpqSBERCQlFYSIiKSkghARkZT+P3ABCJ4SD589AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "betas_x = [1/TOTAL_VOCAB, .0001, .001, .004, .01, .1, 1]\n",
    "accuracies_y = [.856, .87, .873, .877, .874, .808, .59]\n",
    "\n",
    "plt.semilogx(betas_x, accuracies_y, 'black')\n",
    "plt.xlabel('Beta')\n",
    "plt.ylabel('Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
