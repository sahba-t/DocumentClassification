{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import math\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load csv into dense matrix, then converts it to sparse matrix\n",
    "If \"res/sparse_training.data\" exists, **DO NOT** run cell as it will take awhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_matrix():\n",
    "    dense_matrix = np.zeros(shape=(21, 61188), dtype=np.int16)\n",
    "    with open('../res/training.csv', 'r') as train_stream:\n",
    "        for i, line in enumerate(train_stream):\n",
    "            line_int = np.array(list(map(int, line.split(','))), dtype=np.int16)\n",
    "            doc_label = line_int[-1]\n",
    "            dense_matrix[doc_label] += line_int[1:-1]\n",
    "            print(i)\n",
    "        sparse_training = sparse.csr_matrix(dense_matrix)\n",
    "        sparse.save_npz('../res/sparse_training.data',sparse_training)\n",
    "        \n",
    "# load_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sparse_training_data = sparse.load_npz('../res/sparse_training.data.npz')\n",
    "# print(sparse_training_data[21, 3])  \n",
    "print(sparse_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating global vars and consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "UNIQUE_VOCAB = 61188\n",
    "TOTAL_VOCAB = sparse_training_data.sum()\n",
    "BETA = 1/TOTAL_VOCAB\n",
    "ALPHA = 1 + BETA\n",
    "\n",
    "# the pxs used for mutual information\n",
    "pxs = np.zeros(61188, dtype=np.float)\n",
    "pxs_initialized=False\n",
    "\n",
    "set_list = [set() for x in range(0, 20)]\n",
    "class_row_dict = dict(zip(list(range(1, 21)), set_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Priors and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def count_priors():\n",
    "    word_id_ranges = list(range(1, 61189))\n",
    "    column_names =  ['doc_id'] + word_id_ranges + ['label']\n",
    "    p_counts = np.zeros(21, dtype=np.int16)\n",
    "    for i, data_chunk in enumerate(pd.read_csv('../res/training.csv', header=None, chunksize=200, names=column_names, usecols=['label'])): \n",
    "        for _, row in data_chunk.iterrows():\n",
    "            current_label = row['label']\n",
    "            class_row_dict[current_label].add(i)\n",
    "            i += 1\n",
    "    \n",
    "    for j in range(1, 21):\n",
    "        p_counts[j] = len(class_row_dict[j])\n",
    "    \n",
    "    return p_counts / p_counts.sum()\n",
    "\n",
    "prior_counts = count_priors()\n",
    "print(prior_counts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell calculates the mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the p(X=xi) for all words\n",
    "def create_px():\n",
    "    global pxs_initialized\n",
    "    if not pxs_initialized:\n",
    "        print('generating all pxs this takes a few seconds')\n",
    "        for i in range(61188):\n",
    "            pxs[i] = sparse_training_data[:,i].sum()/TOTAL_VOCAB\n",
    "        pxs_initialized = True\n",
    "        print('pxs calculation complete!')    \n",
    "    else:\n",
    "        print('pxs were calculated before. returning the result')\n",
    "        \n",
    "# calculates the mutual information for all categories using the formula\n",
    "# MI = P(x,y) + log(p(x,y)/(p(x).p(y)))\n",
    "def calc_mutual():\n",
    "    create_px()\n",
    "    mutual_info = sparse_training_data.copy()\n",
    "    mutual_results = sparse.csr_matrix(np.zeros((21,61188)), dtype=np.float32)\n",
    "    non_zero_dict={}\n",
    "    for i in range(1,21):\n",
    "        row = mutual_info[i]\n",
    "        total_words = row.sum()\n",
    "        # print(row.shape)\n",
    "        non_zeros = mutual_info[i].nonzero()[1]\n",
    "        non_zero_dict[i] = non_zeros\n",
    "        # print(non_zeros)\n",
    "        # p(x|y)\n",
    "        pxgy = row.multiply(1/total_words)\n",
    "        pxy = pxgy.multiply(prior_counts[i])\n",
    "        pxgy[0,non_zeros] = np.log1p(pxgy[0,non_zeros].multiply(1/pxs[non_zeros]))\n",
    "        mutual_results[i, non_zeros] = pxy[0, non_zeros] + pxgy[0, non_zeros]\n",
    "        print(mutual_results[i, non_zeros[1:4]])\n",
    "    return mutual_results\n",
    "\n",
    "def print_mutual_results(mutual_results, vocab_df, doc_class_df, top_words=5):\n",
    "    results_dense =  mutual_results.todense()\n",
    "    best_words=[\"x\"] * top_words\n",
    "    worst_words = [\"x\"] * top_words\n",
    "    for i in range(1, 21):\n",
    "        row_dense = results_dense[i]\n",
    "        arg_maxes = row_dense.argsort()\n",
    "        print(doc_class_df.iloc[i-1]['doc_class'])\n",
    "        best_indices = arg_maxes[0,-top_words:][::-1]\n",
    "        worst_indices = arg_maxes[0,:top_words]\n",
    "        for ii in range(top_words):\n",
    "            best_index = best_indices[0,ii]\n",
    "            worst_index = worst_indices[0,ii]\n",
    "            best_words[ii] = vocab_df.iloc[best_index].word\n",
    "            worst_words[ii] = vocab_df.iloc[worst_index].word\n",
    "        print('most useful words')\n",
    "        print(best_words)\n",
    "        print('least useful words')\n",
    "        print(worst_words)\n",
    "\n",
    "mutual_results = calc_mutual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The foloowing cell analyzes the mutual information calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_df = pd.read_csv('../res/vocabulary.txt', sep='\\n', header=None, names=[\"word\"])\n",
    "doc_class_df = pd.read_csv('../res/newsgrouplabels.txt', sep='\\n', header=None, names=[\"doc_class\"])\n",
    "#loading\n",
    "print_mutual_results(mutual_results, vocab_df, doc_class_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes\n",
    "formula from the proj2 PDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def multinomial_naive_bayes(row, row_total_words, alpha, debug_prints = False)->int:\n",
    "    if debug_prints: print(row_total_words, row)\n",
    "        \n",
    "    map_denom = np.zeros(21, dtype = np.float64)\n",
    "    for i in range(1, 21):\n",
    "        map_denom[i] = sparse_training_data[i].sum() + ((alpha - 1) * TOTAL_VOCAB)\n",
    "        \n",
    "    k = alpha - 1\n",
    "    max_prob_class = [float('-inf'), -1]\n",
    "    for doc_label in range(1, 21):\n",
    "        running_sum = 0\n",
    "        for word_i, num_words_at_i in row:\n",
    "            running_sum += np.log2((sparse_training_data[doc_label, word_i] + k)/map_denom[doc_label])\n",
    "        posterior = running_sum + np.log2(prior_counts[doc_label])\n",
    "        if posterior > max_prob_class[0]:\n",
    "            max_prob_class[0] = posterior\n",
    "            max_prob_class[1] = doc_label\n",
    "\n",
    "    if debug_prints: print(max_prob_class)\n",
    "    return max_prob_class[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bernoulli_naive_bayes(row, row_total_words, alpha, debug_prints = False)->int:\n",
    "    if debug_prints: print(row_total_words, row)\n",
    "    \n",
    "    # num_zeros = UNIQUE_VOCAB - len(row)\n",
    "    max_prob_class = [float('-inf'), -1]\n",
    "    # k = alpha - 1\n",
    "\n",
    "    # prob_not_appearing = (0+k) / denom\n",
    "    # # prob_of_doc = 1/20  # Not needed because all will be multiplied by it\n",
    "    # unique_words  = 0\n",
    "    for doc_label in range(1, 21):\n",
    "        running_sum = 0\n",
    "        # for word_i, num_words_at_i in row:\n",
    "        #     pass\n",
    "        \n",
    "        posterior = running_sum\n",
    "        if posterior > max_prob_class[0]:\n",
    "            max_prob_class[0] = posterior\n",
    "            max_prob_class[1] = doc_label\n",
    "\n",
    "    if debug_prints: print(max_prob_class)\n",
    "    return max_prob_class[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_training(bayes_function, alpha):\n",
    "    sparse_matrix = sparse.load_npz('../res/nb_training_data.npz')\n",
    "    training_data_coo = sparse_matrix.tocoo()\n",
    "    \n",
    "    row_total_words = training_data_coo.A.sum(axis=1)\n",
    "    correct = 0\n",
    "    row = []\n",
    "    row_i = 0\n",
    "    for row_i, word_i, val in zip(training_data_coo.row, training_data_coo.col, training_data_coo.data):\n",
    "        if word_i != 61188:\n",
    "            \n",
    "            row.append((word_i, val))\n",
    "        else:\n",
    "            classification = bayes_function(row, row_total_words[row_i] - val, alpha=alpha)\n",
    "            correct += 1 if classification == val else 0\n",
    "            row.clear()\n",
    "            if not row_i % 500:\n",
    "                print('At row:', row_i)\n",
    "    print(\"Finished\")\n",
    "    print(f\"accuracy: {(correct / row_i) * 100}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification and Writing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_and_write(bayes_function, write_path, alpha):\n",
    "    sparse_matrix = sparse.load_npz('../res/nb_testing_data.npz')\n",
    "    testing_data_coo = sparse_matrix.tocoo()\n",
    "    with open(write_path, 'w') as out_stream:\n",
    "        out_stream.write(\"id,class\\n\")\n",
    "    \n",
    "        row_total_words = testing_data_coo.A.sum(axis=1)\n",
    "        row_offset = 12000\n",
    "        current_row = 12000\n",
    "        row = []\n",
    "        for row_i, word_i, num_words_at_i in zip(testing_data_coo.row + row_offset, testing_data_coo.col, testing_data_coo.data):\n",
    "            if row_i == current_row:\n",
    "                row.append((word_i, num_words_at_i))\n",
    "            else:\n",
    "                predicted_label = bayes_function(row, row_total_words[row_i - row_offset], alpha=alpha)\n",
    "                out_stream.write(f'{row_i},{predicted_label}\\n')\n",
    "                row.clear()\n",
    "                current_row = row_i\n",
    "                if not current_row % 500:\n",
    "                    print('At row:', current_row)\n",
    "    print(\"File written\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing\n",
    "- multinomial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "b = .004 # BETA: Tried -> 1/TOTAL_VOCAB, .00001, .0001, .001, .01, .1, 1\n",
    "a = 1 + b  # ALPHA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "print('beta:', b)\n",
    "test_training(multinomial_naive_bayes, alpha=a)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60, 'minutes.') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "print('beta:', b)\n",
    "file_path = '../results/multinomial_NB_b' + str(b)[(2 if b < 1 else 0) :] +'_results.csv'\n",
    "test_and_write(multinomial_naive_bayes, file_path, alpha=a)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60, 'minutes.') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Bernoulli"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "print('beta:', b)\n",
    "test_training(bernoulli_naive_bayes, alpha=a)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60, 'minutes.') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "print('beta:', b)\n",
    "file_path = '../results/bernoulli_NB_b' + str(b)[(2 if b < 1 else 0) :] +'_results.csv'\n",
    "test_and_write(bernoulli_naive_bayes, file_path, alpha=a)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60, 'minutes.') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}