{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import math\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load csv into dense matrix, then converts it to sparse matrix\n",
    "If \"res/sparse_training.data\" exists, **DO NOT** run cell as it will take awhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dense_matrix = np.zeros(shape=(21, 61188), dtype=np.int16)\n",
    "with open('../res/training.csv', 'r') as train_stream:\n",
    "    for i, line in enumerate(train_stream):\n",
    "        line_int = np.array(list(map(int, line.split(','))), dtype=np.int16)\n",
    "        doc_label = line_int[-1]\n",
    "        dense_matrix[doc_label] += line_int[1:-1]\n",
    "        print(i)\n",
    "    sparse_training = sparse.csr_matrix(dense_matrix)\n",
    "    sparse.save_npz('../res/sparse_training.data',sparse_training)\n",
    "    print(sparse_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 0)\t9\n",
      "  (1, 1)\t53\n",
      "  (1, 2)\t237\n",
      "  (1, 3)\t11\n",
      "  (1, 4)\t48\n",
      "  (1, 5)\t36\n",
      "  (1, 6)\t7\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t31\n",
      "  (1, 9)\t127\n",
      "  (1, 10)\t5\n",
      "  (1, 11)\t4336\n",
      "  (1, 12)\t19\n",
      "  (1, 13)\t24\n",
      "  (1, 14)\t46\n",
      "  (1, 15)\t597\n",
      "  (1, 16)\t267\n",
      "  (1, 17)\t12\n",
      "  (1, 18)\t9\n",
      "  (1, 19)\t15\n",
      "  (1, 20)\t2\n",
      "  (1, 21)\t3\n",
      "  (1, 22)\t2985\n",
      "  (1, 23)\t4\n",
      "  (1, 24)\t325\n",
      "  :\t:\n",
      "  (20, 61146)\t1\n",
      "  (20, 61147)\t1\n",
      "  (20, 61148)\t1\n",
      "  (20, 61149)\t2\n",
      "  (20, 61150)\t1\n",
      "  (20, 61151)\t1\n",
      "  (20, 61152)\t1\n",
      "  (20, 61153)\t1\n",
      "  (20, 61154)\t1\n",
      "  (20, 61169)\t2\n",
      "  (20, 61170)\t2\n",
      "  (20, 61171)\t2\n",
      "  (20, 61172)\t3\n",
      "  (20, 61173)\t4\n",
      "  (20, 61174)\t2\n",
      "  (20, 61175)\t3\n",
      "  (20, 61176)\t6\n",
      "  (20, 61177)\t1\n",
      "  (20, 61178)\t2\n",
      "  (20, 61179)\t2\n",
      "  (20, 61183)\t2\n",
      "  (20, 61184)\t2\n",
      "  (20, 61185)\t2\n",
      "  (20, 61186)\t2\n",
      "  (20, 61187)\t2\n"
     ]
    }
   ],
   "source": [
    "sparse_training_data = sparse.load_npz('../res/sparse_training.data.npz')\n",
    "# print(sparse_training_data[21, 3])  \n",
    "print(sparse_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating global vars and consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "UNIQUE_VOCAB = 61188\n",
    "TOTAL_VOCAB = sparse_training_data.sum()\n",
    "BETA = 1/TOTAL_VOCAB\n",
    "ALPHA = 1 + BETA\n",
    "\n",
    "# the pxs used for mutual information\n",
    "pxs = np.zeros(61188, dtype=np.float)\n",
    "pxs_initialized=False\n",
    "\n",
    "set_list = [set() for x in range(0, 20)]\n",
    "class_row_dict = dict(zip(list(range(1, 21)), set_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Priors and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.04025    0.052      0.05183333 0.05358333 0.05016667\n",
      " 0.0525     0.0515     0.05116667 0.05408333 0.05233333 0.05383333\n",
      " 0.05325    0.05216667 0.05175    0.05308333 0.05425    0.04833333\n",
      " 0.04941667 0.03891667 0.03558333]\n"
     ]
    }
   ],
   "source": [
    "word_id_ranges = list(range(1, 61189))\n",
    "column_names =  ['doc_id'] + word_id_ranges + ['label']\n",
    "prior_counts = np.zeros(21, dtype=np.int16)\n",
    "i = 0\n",
    "for data_chunk in pd.read_csv('../res/training.csv', header=None, chunksize=200, names=column_names, usecols=['label']): \n",
    "    for _, row in data_chunk.iterrows():\n",
    "        current_label = row['label']\n",
    "        class_row_dict[current_label].add(i)\n",
    "        i += 1\n",
    "\n",
    "for j in range(1, 21):\n",
    "    prior_counts[j] = len(class_row_dict[j])\n",
    "\n",
    "prior_counts = prior_counts / prior_counts.sum()\n",
    "print(prior_counts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "formula from the proj2 PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def multinomial_naive_bayes(row, row_total_words, debug_prints = False)->int:\n",
    "    if debug_prints: print(row_total_words, row)\n",
    "        \n",
    "    map_denom = np.zeros(21, dtype = np.float64)\n",
    "    for i in range(1, 21):\n",
    "        map_denom[i] = sparse_training_data[i].sum() + ((ALPHA - 1) * TOTAL_VOCAB)\n",
    "        \n",
    "    k = ALPHA - 1\n",
    "    max_prob_class = [-math.inf, -1]\n",
    "    for doc_label in range(1, 21):\n",
    "        running_sum = 0\n",
    "        for word_i, num_words_at_i in row:\n",
    "            running_sum += math.log2((sparse_training_data[doc_label, word_i] + k)/map_denom[doc_label])\n",
    "        posterior = running_sum + math.log2(prior_counts[doc_label])\n",
    "        if posterior > max_prob_class[0]:\n",
    "            max_prob_class[0] = posterior\n",
    "            max_prob_class[1] = doc_label\n",
    "\n",
    "    if debug_prints: print(max_prob_class)\n",
    "    return max_prob_class[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bernoulli_naive_bayes(row, row_total_words, debug_prints = False)->int:\n",
    "    if debug_prints: print(row_total_words, row)\n",
    "\n",
    "    num_zeros = UNIQUE_VOCAB - len(row)\n",
    "    max_prob_class = [-math.inf, -1]\n",
    "    k = ALPHA - 1\n",
    "\n",
    "    # prob_not_appearing = (0+k) / denom\n",
    "    # # prob_of_doc = 1/20  # Not needed because all will be multiplied by it\n",
    "    # unique_words  = 0\n",
    "    for doc_label in range(1, 21):\n",
    "       \n",
    "        posterior = 0\n",
    "        if posterior > max_prob_class[0]:\n",
    "            max_prob_class[0] = posterior\n",
    "            max_prob_class[1] = doc_label\n",
    "\n",
    "    if debug_prints: print(max_prob_class)\n",
    "    return max_prob_class[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell calculates the mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pxs were calculated before. returning the result\n",
      "  (0, 0)\t0.5675118\n",
      "  (0, 1)\t2.8093176\n",
      "  (0, 2)\t0.54314953\n",
      "  (0, 0)\t0.8404472\n",
      "  (0, 1)\t1.0898556\n",
      "  (0, 2)\t1.2172484\n",
      "  (0, 0)\t0.7652431\n",
      "  (0, 1)\t0.08386552\n",
      "  (0, 2)\t1.2357646\n",
      "  (0, 0)\t0.67656314\n",
      "  (0, 1)\t0.09330114\n",
      "  (0, 2)\t0.7075954\n",
      "  (0, 0)\t0.6471796\n",
      "  (0, 1)\t0.36999592\n",
      "  (0, 2)\t0.10172739\n",
      "  (0, 0)\t1.3976736\n",
      "  (0, 1)\t2.0527852\n",
      "  (0, 2)\t0.66137034\n",
      "  (0, 0)\t0.2745513\n",
      "  (0, 1)\t0.6281791\n",
      "  (0, 2)\t0.9207785\n",
      "  (0, 0)\t0.65871525\n",
      "  (0, 1)\t0.17273226\n",
      "  (0, 2)\t0.3080399\n",
      "  (0, 0)\t0.7338978\n",
      "  (0, 1)\t0.3112273\n",
      "  (0, 2)\t0.29957005\n",
      "  (0, 0)\t0.7456222\n",
      "  (0, 1)\t0.1506598\n",
      "  (0, 2)\t0.07490105\n",
      "  (0, 0)\t0.76581794\n",
      "  (0, 1)\t0.1323618\n",
      "  (0, 2)\t1.2368948\n",
      "  (0, 0)\t0.6758495\n",
      "  (0, 1)\t0.38966617\n",
      "  (0, 2)\t1.2762351\n",
      "  (0, 0)\t0.18796521\n",
      "  (0, 1)\t0.40184343\n",
      "  (0, 2)\t0.370503\n",
      "  (0, 0)\t0.43567878\n",
      "  (0, 1)\t0.63758093\n",
      "  (0, 2)\t1.0397121\n",
      "  (0, 0)\t0.62194014\n",
      "  (0, 1)\t0.9865969\n",
      "  (0, 2)\t0.21381891\n",
      "  (0, 0)\t0.88221705\n",
      "  (0, 1)\t0.41422883\n",
      "  (0, 2)\t0.47853354\n",
      "  (0, 0)\t0.30871767\n",
      "  (0, 1)\t0.30207914\n",
      "  (0, 2)\t0.48152572\n",
      "  (0, 0)\t0.72878236\n",
      "  (0, 1)\t0.035635605\n",
      "  (0, 2)\t0.40954068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n",
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.05717618\n",
      "  (0, 1)\t0.80262434\n",
      "  (0, 2)\t0.69800466\n",
      "  (0, 0)\t0.49419263\n",
      "  (0, 1)\t0.67950296\n",
      "  (0, 2)\t0.13902645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahba/programFiles/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:118: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#calculates the p(X=xi) for all words\n",
    "def create_px():\n",
    "    global pxs_initialized\n",
    "    if not pxs_initialized:\n",
    "        print('generating all pxs this takes a few seconds')\n",
    "        for i in range(61188):\n",
    "            pxs[i] = sparse_training_data[:,i].sum()/TOTAL_VOCAB\n",
    "        pxs_initialized = True\n",
    "        print('pxs calculation complete!')    \n",
    "    else:\n",
    "        print('pxs were calculated before. returning the result')\n",
    "# calculates the mutual information for all categories using the formula\n",
    "# MI = P(x,y) + log(p(x,y)/(p(x).p(y)))\n",
    "def calc_mutual():\n",
    "    create_px()\n",
    "    mutual_info = sparse_training_data.copy()\n",
    "    mutual_results = sparse.csr_matrix(np.zeros((21,61188)), dtype=np.float32)\n",
    "    non_zero_dict={}\n",
    "    for i in range(1,21):\n",
    "        row = mutual_info[i]\n",
    "        total_words = row.sum()\n",
    "        # print(row.shape)\n",
    "        non_zeros = mutual_info[i].nonzero()[1]\n",
    "        non_zero_dict[i] = non_zeros\n",
    "        # print(non_zeros)\n",
    "        # p(x|y)\n",
    "        pxgy = row.multiply(1/total_words)\n",
    "        pxy = pxgy.multiply(prior_counts[i])\n",
    "        pxgy[0,non_zeros] = np.log1p(pxgy[0,non_zeros].multiply(1/pxs[non_zeros]))\n",
    "        mutual_results[i, non_zeros] = pxy[0, non_zeros] + pxgy[0, non_zeros]\n",
    "        print(mutual_results[i, non_zeros[1:4]])\n",
    "    return mutual_results\n",
    "\n",
    "def print_mutual_results(mutual_results, vocab_df, doc_class_df, top_words=5):\n",
    "    results_dense =  mutual_results.todense()\n",
    "    best_words=[\"x\"] * top_words\n",
    "    worst_words = [\"x\"] * top_words\n",
    "    for i in range(1, 21):\n",
    "        row_dense = results_dense[i]\n",
    "        arg_maxes = row_dense.argsort()\n",
    "        print(doc_class_df.iloc[i-1]['doc_class'])\n",
    "        best_indices = arg_maxes[0,-top_words:][::-1]\n",
    "        worst_indices = arg_maxes[0,:top_words]\n",
    "        for ii in range(top_words):\n",
    "            best_index = best_indices[0,ii]\n",
    "            worst_index = worst_indices[0,ii]\n",
    "            best_words[ii] = vocab_df.iloc[best_index].word\n",
    "            worst_words[ii] = vocab_df.iloc[worst_index].word\n",
    "        print('most useful words')\n",
    "        print(best_words)\n",
    "        print('least useful words')\n",
    "        print(worst_words)\n",
    "\n",
    "\n",
    "mutual_results = calc_mutual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The foloowing cell analyzes the mutual information calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 alt.atheism\n",
      "most useful words\n",
      "['darice', 'buphy', 'bobbe', 'beauchaine', 'mozumder']\n",
      "least useful words\n",
      "['vijay', 'elaine', 'magnetometer', 'typ', 'jjj']\n",
      "-------------------------------\n",
      "2 comp.graphics\n",
      "most useful words\n",
      "['nfotis', 'radiosity', 'siggraph', 'pov', 'rayshade']\n",
      "least useful words\n",
      "['vijay', 'scuttle', 'definetly', 'rectifies', 'chops']\n",
      "-------------------------------\n",
      "3 comp.os.ms-windows.misc\n",
      "most useful words\n",
      "['ashok', 'louray', 'vgalogo', 'qw', 'rlk']\n",
      "least useful words\n",
      "['vijay', 'recommending', 'coords', 'microwaves', 'surveyors']\n",
      "-------------------------------\n",
      "4 comp.sys.ibm.pc.hardware\n",
      "most useful words\n",
      "['fasst', 'jmarttila', 'usma', 'rll', 'fdisk']\n",
      "least useful words\n",
      "['vijay', 'emisions', 'eaton', 'woodard', 'juhan']\n",
      "-------------------------------\n",
      "5 comp.sys.mac.hardware\n",
      "most useful words\n",
      "['adb', 'iivx', 'bmug', 'lciii', 'iisi']\n",
      "least useful words\n",
      "['vijay', 'demondulator', 'downconversion', 'ackermann', 'inverting']\n",
      "-------------------------------\n",
      "6 comp.windows.x\n",
      "most useful words\n",
      "['argv', 'oname', 'ndet', 'xdm', 'args']\n",
      "least useful words\n",
      "['vijay', 'rafters', 'entrances', 'nmw', 'furnace']\n",
      "-------------------------------\n",
      "7 misc.forsale\n",
      "most useful words\n",
      "['keown', 'punisher', 'liefeld', 'sabretooth', 'wolverine']\n",
      "least useful words\n",
      "['archive', 'poeldvere', 'jakobi', 'estonia', 'conductance']\n",
      "-------------------------------\n",
      "8 rec.autos\n",
      "most useful words\n",
      "['buick', 'infiniti', 'corvette', 'opel', 'geico']\n",
      "least useful words\n",
      "['etrbom', 'recchi', 'fedyk', 'galley', 'eklund']\n",
      "-------------------------------\n",
      "9 rec.motorcycles\n",
      "most useful words\n",
      "['blaine', 'motorcycling', 'cookson', 'countersteering', 'biker']\n",
      "least useful words\n",
      "['vijay', 'propranolol', 'psychogenic', 'lungs', 'inmet']\n",
      "-------------------------------\n",
      "10 rec.sport.baseball\n",
      "most useful words\n",
      "['hirschbeck', 'mattingly', 'rbi', 'alomar', 'pitcher']\n",
      "least useful words\n",
      "['vijay', 'inkjets', 'downloadable', 'cablevision', 'dissimilar']\n",
      "-------------------------------\n",
      "11 rec.sport.hockey\n",
      "most useful words\n",
      "['ahl', 'bruins', 'nyr', 'leafs', 'nhl']\n",
      "least useful words\n",
      "['vijay', 'chiggers', 'affliction', 'gainesville', 'chigger']\n",
      "-------------------------------\n",
      "12 sci.crypt\n",
      "most useful words\n",
      "['cryptographic', 'plaintext', 'denning', 'crypt', 'ripem']\n",
      "least useful words\n",
      "['vijay', 'psychiatric', 'enacted', 'neurology', 'neurologist']\n",
      "-------------------------------\n",
      "13 sci.electronics\n",
      "most useful words\n",
      "['deaddio', 'capacitors', 'bubblejets', 'cmkrnl', 'adcom']\n",
      "least useful words\n",
      "['archive', 'nfpa', 'schwan', 'vacate', 'inspectors']\n",
      "-------------------------------\n",
      "14 sci.med\n",
      "most useful words\n",
      "['antibiotic', 'antibiotics', 'hicnet', 'dyer', 'candida']\n",
      "least useful words\n",
      "['vijay', 'cryptophone', 'cylink', 'laissez', 'blames']\n",
      "-------------------------------\n",
      "15 sci.space\n",
      "most useful words\n",
      "['ssto', 'prb', 'nsmca', 'baalke', 'orbiter']\n",
      "least useful words\n",
      "['vijay', 'keyspaces', 'disctribution', 'usenetters', 'lawfulness']\n",
      "-------------------------------\n",
      "16 soc.religion.christian\n",
      "most useful words\n",
      "['ahmadiyya', 'arsenokoitai', 'jayne', 'clh', 'athos']\n",
      "least useful words\n",
      "['archive', 'casserole', 'tlu', 'gic', 'ethridge']\n",
      "-------------------------------\n",
      "17 talk.politics.guns\n",
      "most useful words\n",
      "['shotguns', 'rutledge', 'thomasp', 'kratz', 'homicides']\n",
      "least useful words\n",
      "['vijay', 'slogans', 'shiriff', 'speculates', 'selectively']\n",
      "-------------------------------\n",
      "18 talk.politics.mideast\n",
      "most useful words\n",
      "['gaza', 'serdar', 'israelis', 'argic', 'istanbul']\n",
      "least useful words\n",
      "['vijay', 'redpoll', 'depew', 'corrosive', 'eichener']\n",
      "-------------------------------\n",
      "19 talk.politics.misc\n",
      "most useful words\n",
      "['gordian', 'locutus', 'hendricks', 'steveh', 'stephanopoulos']\n",
      "least useful words\n",
      "['archive', 'behavioural', 'jhan', 'dgbt', 'vunerable']\n",
      "-------------------------------\n",
      "20 talk.religion.misc\n",
      "most useful words\n",
      "['caligiuri', 'mcconkie', 'zoroastrians', 'zarathushtra', 'royalroads']\n",
      "least useful words\n",
      "['vijay', 'beersheba', 'pasture', 'derby', 'sscl']\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "vocab_df = pd.read_csv('../res/vocabulary.txt', sep='\\n', header=None, names=[\"word\"])\n",
    "doc_class_df = pd.read_csv('../res/newsgrouplabels.txt', sep='\\n', header=None, names=[\"doc_class\"])\n",
    "#loading\n",
    "print_mutual_results(mutual_results, vocab_df, doc_class_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training  and Testing Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_training(bayes_function):\n",
    "    sparse_matrix = sparse.load_npz('../res/nb_training_data.npz')\n",
    "    training_data_coo = sparse_matrix.tocoo()\n",
    "    \n",
    "    row_total_words = training_data_coo.A.sum(axis=1)\n",
    "    correct = 0\n",
    "    row = []\n",
    "    for row_i, word_i, val in zip(training_data_coo.row, training_data_coo.col, training_data_coo.data):\n",
    "        if word_i != 61188:\n",
    "            row.append((word_i, val))\n",
    "        else:\n",
    "            classification = bayes_function(row, row_total_words[row_i] - val)\n",
    "            correct += 1 if classification == val else 0\n",
    "            row.clear()\n",
    "            if not row_i % 200:\n",
    "                print('At row:', row_i)\n",
    "    print(\"Finished\")\n",
    "    print(f\"accuracy: {(correct / row_i) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_and_write(bayes_function, write_path):\n",
    "    sparse_matrix = sparse.load_npz('../res/nb_testing_data.npz')\n",
    "    testing_data_coo = sparse_matrix.tocoo()\n",
    "    with open(write_path, 'w') as out_stream:\n",
    "        out_stream.write(\"id,class\\n\")\n",
    "    \n",
    "        row_total_words = testing_data_coo.A.sum(axis=1)\n",
    "        row_offset = 12000\n",
    "        current_row = 12000\n",
    "        row = []\n",
    "        for row_i, word_i, num_words_at_i in zip(testing_data_coo.row + row_offset, testing_data_coo.col, testing_data_coo.data):\n",
    "            if row_i == current_row:\n",
    "                row.append((word_i, num_words_at_i))\n",
    "            else:\n",
    "                predicted_label = bayes_function(row, row_total_words[row_i - row_offset])\n",
    "                out_stream.write(f'{row_i},{predicted_label}\\n')\n",
    "                row.clear()\n",
    "                current_row = row_i\n",
    "                if not current_row % 200:\n",
    "                    print('At row:', current_row)\n",
    "    print(\"File written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing\n",
    "- multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n",
      "accuracy: 99.02491874322861%\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "test_training(multinomial_naive_bayes)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60, 'minutes.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At row: 12200\n",
      "At row: 12400\n",
      "At row: 12600\n",
      "At row: 12800\n",
      "At row: 13000\n",
      "At row: 13200\n",
      "At row: 13400\n",
      "At row: 13600\n",
      "At row: 13800\n",
      "At row: 14000\n",
      "At row: 14200\n",
      "At row: 14400\n",
      "At row: 14600\n",
      "At row: 14800\n",
      "At row: 15000\n",
      "At row: 15200\n",
      "At row: 15400\n",
      "At row: 15600\n",
      "At row: 15800\n",
      "At row: 16000\n",
      "At row: 16200\n",
      "At row: 16400\n",
      "At row: 16600\n",
      "At row: 16800\n",
      "At row: 17000\n",
      "At row: 17200\n",
      "At row: 17400\n",
      "At row: 17600\n",
      "At row: 17800\n",
      "At row: 18000\n",
      "At row: 18200\n",
      "At row: 18400\n",
      "At row: 18600\n",
      "File written\n",
      "Time:  966.1413919999977\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "test_and_write(multinomial_naive_bayes, '../results/multinomial_NB_results.csv')\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60, 'minutes.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "test_training(bernoulli_naive_bayes)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60, 'minutes.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "test_and_write(bernoulli_naive_bayes, '../results/bernoulli_NB_results.csv')\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', (stop - start) / 60, 'minutes.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61189\n",
      "61189\n",
      "61189\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# with open('../res/testing.csv', 'r') as train_stream:\n",
    "#     count = 0\n",
    "#     for i, line in enumerate(train_stream):\n",
    "#         current_line = np.array(list(map(int, line.split(','))), dtype=np.int16)\n",
    "#         print(len(current_line))\n",
    "#         if i == 2:\n",
    "#             break\n",
    "#     print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
