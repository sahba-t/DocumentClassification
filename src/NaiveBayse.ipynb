{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load csv into dense matrix, then converts it to sparse matrix\n",
    "#### If \"res/sparse_training.data\" exists, DO NOT run cell as it will take awhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dense_matrix = np.zeros(shape=(21, 61188), dtype=np.int16)\n",
    "with open('../res/training.csv', 'r') as train_strem:\n",
    "    i=0\n",
    "    for line in train_strem:\n",
    "        line_int = np.array(list(map(int, line.split(','))), dtype=np.int16)\n",
    "        doc_label = line_int[-1]\n",
    "        dense_matrix[doc_label] += line_int[1:-1]\n",
    "        i += 1\n",
    "        print(i)\n",
    "    sparse_training = sparse.csr_matrix(dense_matrix)\n",
    "    sparse.save_npz('../res/sparse_training.data',sparse_training)\n",
    "    print(sparse_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "  (1, 0)\t9\n",
      "  (1, 1)\t53\n",
      "  (1, 2)\t237\n",
      "  (1, 3)\t11\n",
      "  (1, 4)\t48\n",
      "  (1, 5)\t36\n",
      "  (1, 6)\t7\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t31\n",
      "  (1, 9)\t127\n",
      "  (1, 10)\t5\n",
      "  (1, 11)\t4336\n",
      "  (1, 12)\t19\n",
      "  (1, 13)\t24\n",
      "  (1, 14)\t46\n",
      "  (1, 15)\t597\n",
      "  (1, 16)\t267\n",
      "  (1, 17)\t12\n",
      "  (1, 18)\t9\n",
      "  (1, 19)\t15\n",
      "  (1, 20)\t2\n",
      "  (1, 21)\t3\n",
      "  (1, 22)\t2985\n",
      "  (1, 23)\t4\n",
      "  (1, 24)\t325\n",
      "  :\t:\n",
      "  (20, 61146)\t1\n",
      "  (20, 61147)\t1\n",
      "  (20, 61148)\t1\n",
      "  (20, 61149)\t2\n",
      "  (20, 61150)\t1\n",
      "  (20, 61151)\t1\n",
      "  (20, 61152)\t1\n",
      "  (20, 61153)\t1\n",
      "  (20, 61154)\t1\n",
      "  (20, 61169)\t2\n",
      "  (20, 61170)\t2\n",
      "  (20, 61171)\t2\n",
      "  (20, 61172)\t3\n",
      "  (20, 61173)\t4\n",
      "  (20, 61174)\t2\n",
      "  (20, 61175)\t3\n",
      "  (20, 61176)\t6\n",
      "  (20, 61177)\t1\n",
      "  (20, 61178)\t2\n",
      "  (20, 61179)\t2\n",
      "  (20, 61183)\t2\n",
      "  (20, 61184)\t2\n",
      "  (20, 61185)\t2\n",
      "  (20, 61186)\t2\n",
      "  (20, 61187)\t2\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "sparse_training_data = sparse.load_npz('../res/sparse_training.data.npz')\n",
    "print(sparse_training_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating global vars"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set_list = [set() for x in range(0, 20)]\n",
    "class_row_dict = dict(zip(list(range(1, 21)), set_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Counting Priors and words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[0.         0.04025    0.052      0.05183333 0.05358333 0.05016667\n",
      " 0.0525     0.0515     0.05116667 0.05408333 0.05233333 0.05383333\n",
      " 0.05325    0.05216667 0.05175    0.05308333 0.05425    0.04833333\n",
      " 0.04941667 0.03891667 0.03558333]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "word_id_ranges = list(range(1, 61189))\n",
    "column_names =  ['doc_id'] + word_id_ranges + ['label']\n",
    "prior_counts = np.zeros(21, dtype=np.int16)\n",
    "i = 0\n",
    "for data_chunk in pd.read_csv('../res/training.csv', header=None, chunksize=200, names=column_names, usecols=['label']): \n",
    "    for _, row in data_chunk.iterrows():\n",
    "        current_label = row['label']\n",
    "        class_row_dict[current_label].add(i)\n",
    "        i += 1\n",
    "\n",
    "for j in range(1, 21):\n",
    "    prior_counts[j] = len(class_row_dict[j])\n",
    "\n",
    "prior_counts = prior_counts / prior_counts.sum()\n",
    "print(prior_counts) "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes formula from the proj2 PDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "uniq_vocab = 61188\n",
    "total_vocab = sparse_training_data.sum()\n",
    "alpha = 1 + 1/total_vocab\n",
    "denom = np.zeros(21, dtype = np.float64)\n",
    "for i in range(1, 21):\n",
    "    denom[i] = sparse_training_data[i].sum() + ((alpha - 1) * total_vocab)\n",
    " \n",
    "do_naive_debug = False \n",
    "def do_naive(row):\n",
    "    max_prob = -math.inf\n",
    "    max_doc_class = -1\n",
    "    non_zero_indices = row.nonzero()[0]\n",
    "    for doc_label in range(1, 21):\n",
    "        running_sum = 0\n",
    "        for word in non_zero_indices:\n",
    "            running_sum += math.log2((sparse_training_data[doc_label,word] + alpha - 1)/denom[doc_label])\n",
    "        new_prob = running_sum + math.log2(prior_counts[doc_label])\n",
    "        if new_prob > max_prob:\n",
    "            max_prob = new_prob\n",
    "            max_doc_class = doc_label\n",
    "    assert(max_doc_class != -1)\n",
    "    if do_naive_debug:\n",
    "        print(f\"{max_doc_class}: {max_prob}\")\n",
    "    return max_doc_class"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# With training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "14:14\n",
      "16:16\n",
      "8:8\n",
      "11:11\n",
      "12:12\n",
      "7:7\n",
      "15:15\n",
      "6:6\n",
      "5:8\n",
      "5:5\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "with open('../res/training.csv', 'r') as test_stream:\n",
    "    i = 1\n",
    "    for line in test_stream:\n",
    "        test_array = np.array(list(map(int,line.split(','))))\n",
    "        doc_id = test_array[0]\n",
    "        doc_label =test_array[-1]\n",
    "        predicted_label = do_naive(test_array[1:-1])\n",
    "        print(f\"{doc_label}:{predicted_label}\")\n",
    "        i += 1\n",
    "        if i > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# With testing data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "with open('../res/testing.csv', 'r') as test_stream, open('../results/out.csv', 'w') as out_stream:\n",
    "    out_stream.write(\"id,class\\n\")\n",
    "    for line in test_stream:\n",
    "        test_array = np.array(list(map(int, line.split(','))))\n",
    "        doc_id =test_array[0]\n",
    "        predicted_label = do_naive(test_array[1:])    \n",
    "        out_stream.write(f\"{doc_id},{predicted_label}\\n\")\n",
    "print(\"File written\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}