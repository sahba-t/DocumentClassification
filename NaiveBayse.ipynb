{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating global containers for holding stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_list = [set() for x in range(0,20)]\n",
    "class_row_dict=dict(zip(list(range(1,21)),set_list))\n",
    "dense_matrix = np.zeros(shape=(21,61188), dtype=np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This has been run before we can simply load the sparse matrix: see_below\n",
    "### This loads the csv into  dense matrix and then converts it to sparse matrix and saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('training.csv', 'r') as train_strem:\n",
    "    i=0\n",
    "    for line in train_strem:\n",
    "        line_int = np.array(list(map(int, line.split(','))), dtype=np.int16)\n",
    "        doc_label = line_int[-1]\n",
    "        dense_matrix[doc_label] += line_int[1:-1]\n",
    "        i += 1\n",
    "        print(i)\n",
    "    sparse_training = sparse.csr_matrix(dense_matrix)\n",
    "    sparse.save_npz('sparse_training.data',sparse_training)\n",
    "    print(sparse_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the testing data into sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "4336\n"
    }
   ],
   "source": [
    "#how load\n",
    "#print(sparse_row_leader)\n",
    "sparse_training_data = sparse.load_npz('sparse_training.data.npz')\n",
    "print(sparse_training_data[1,11])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Counting Priors and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0.         0.04025    0.052      0.05183333 0.05358333 0.05016667\n 0.0525     0.0515     0.05116667 0.05408333 0.05233333 0.05383333\n 0.05325    0.05216667 0.05175    0.05308333 0.05425    0.04833333\n 0.04941667 0.03891667 0.03558333]\n"
    }
   ],
   "source": [
    "word_id_ranges = list(range(1,61189))\n",
    "column_names =  ['doc_id'] + word_id_ranges + ['label']\n",
    "prior_counts = np.zeros(21, dtype=np.int16)\n",
    "#for data_chunk in pd.read_csv('training.csv', header=None, chunksize=200, names=column_names, usecols=['label']):\n",
    "i = 0\n",
    "for data_chunk in pd.read_csv('training.csv', header=None, chunksize=200, names=column_names, usecols=['label']): \n",
    "    for _, row in data_chunk.iterrows():\n",
    "        current_label = row['label']\n",
    "        class_row_dict[current_label].add(i)\n",
    "        i += 1\n",
    "for j in range(1,21):\n",
    "    prior_counts[j] = len(class_row_dict[j])\n",
    "prior_counts = prior_counts / prior_counts.sum()\n",
    "print(prior_counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell implements the actual naive bayse formula from the PDF\n",
    "\n",
    "uniq_vocab = 61188\n",
    "total_vocab = sparse_training_data.sum()\n",
    "alpha = 1 + 1/total_vocab\n",
    "denom = np.zeros(21, dtype = np.float64)\n",
    "for i in range(1,21):\n",
    "    denom[i] = sparse_training_data[i].sum() + ((alpha - 1) * total_vocab)\n",
    " \n",
    "do_naive_debug = False \n",
    "def do_naive(row):\n",
    "    max_prob = -math.inf\n",
    "    max_doc_class = -1\n",
    "    non_zero_indices = row.nonzero()[0]\n",
    "    for doc_label in range(1,21):\n",
    "        running_sum = 0\n",
    "        for word in non_zero_indices:\n",
    "            running_sum += math.log2((sparse_training_data[doc_label,word] + alpha - 1)/denom[doc_label])\n",
    "        new_prob = running_sum + math.log2(prior_counts[doc_label])\n",
    "        if new_prob > max_prob:\n",
    "            max_prob = new_prob\n",
    "            max_doc_class = doc_label\n",
    "    assert(max_doc_class != -1)\n",
    "    if do_naive_debug:\n",
    "        print(f\"{max_doc_class}: {max_prob}\")\n",
    "    return max_doc_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training.csv', 'r') as test_stream:\n",
    "    i = 1\n",
    "    for line in test_stream:\n",
    "        test_array = np.array(list(map(int,line.split(','))))\n",
    "        doc_id = test_array[0]\n",
    "        doc_label =test_array[-1]\n",
    "        predicted_label = do_naive(test_array[1:-1])\n",
    "        print(f\"{doc_label}:{predicted_label}\")\n",
    "        i += 1\n",
    "        if i > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual testing:\n",
    "i = 0\n",
    "with open('testing.csv', 'r') as test_stream:\n",
    "    with open('out.csv', 'w') as out_stream:\n",
    "        out_stream.write(\"id,class\\n\")\n",
    "        for line in test_stream:\n",
    "            test_array = np.array(list(map(int,line.split(','))))\n",
    "            doc_id =test_array[0]\n",
    "            predicted_label = do_naive(test_array[1:])    \n",
    "            out_stream.write(f\"{doc_id},{predicted_label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The cells below are for Experiments only DO NOT RUN THEY MIGHT BLOW UP YOUR LAPTOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_cls_id, row_set in class_row_dict.items():\n",
    "    print(f\"class {doc_cls_id} starting\")\n",
    "    row_leader = row_set.pop()\n",
    "    sparse_row_leader[doc_cls_id] = row_leader\n",
    "    for other_row in row_set:\n",
    "        sparse_data[row_leader] += sparse_data[other_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a sparse matrix\n",
    "with open('training.csv', 'r') as train_strem:\n",
    "    i = 0\n",
    "    for line in train_strem:\n",
    "        line_int = np.array(list(map(int, line.split(','))), dtype=np.int16)\n",
    "        doc_label = line_int[-1]\n",
    "        row_leader = sparse_row_leader[doc_label]\n",
    "        # print(row_leader)\n",
    "        if row_leader == -1:\n",
    "            sparse_row_leader[doc_label] = i\n",
    "            sparse_data[i,:] = line_int[1:-1]\n",
    "        else:\n",
    "            sparse_data[row_leader] += line_int[1:-1] \n",
    "\n",
    "        # sparse_data[i,:] = list(map(int, line.split(',')))[1:-1]\n",
    "        i+=1\n",
    "        print(i)\n",
    "#saving the sparse matrix to file\n",
    "sparse_data = sparse_data.tocsr()\n",
    "sparse.save_npz('sparse.data',sparse_data)\n",
    "with open('row_leader.obj', 'wb') as picke_stream:\n",
    "    pickle.dump(row_leader, picke_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use index 1-> 61188\n",
    "count_data=[{'label':x, 'counts':np.zeros(shape=(61189), dtype=np.int32)} for x in range(1,21)]\n",
    "count_df = pd.DataFrame(count_data, index=list(range(1,21)))\n",
    "print(\"this is what the df looks like\")\n",
    "print(count_df.head())\n",
    "print('This is how we index!')\n",
    "print(count_df.loc[1,'label'])\n",
    "print('accessing the numpuy array')\n",
    "print(type(count_df.loc[1,'counts'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS CELL BUT KEEP IT\n",
    "\n",
    "word_id_ranges = list(range(1,61189))\n",
    "column_names =  ['doc_id'] + word_id_ranges + ['label']\n",
    "prior_counts = [0] * 21\n",
    "#for data_chunk in pd.read_csv('training.csv', header=None, chunksize=200, names=column_names, usecols=['label']):\n",
    "for data_chunk in pd.read_csv('training.csv', header=None, chunksize=200, names=column_names, usecols=['label']): \n",
    "    for _, row in data_chunk.iterrows():\n",
    "        #what document label we are dealing with!?\n",
    "        current_label = row['label']\n",
    "        # So we have seen one more of this type!\n",
    "        prior_counts [current_label] += 1\n",
    "\n",
    "print(prior_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda88c4e0197d3a49879326dc8e8d480baa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}